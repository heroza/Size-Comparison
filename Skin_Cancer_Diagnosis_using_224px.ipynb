{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Size-Comparison/blob/main/Skin_Cancer_Diagnosis_using_224px.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "26aec09c-09a9-4d26-c381-342eac0e42f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_x4c0_DTkaa"
      },
      "source": [
        "#Library, atribut, and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nR2MJBYq-oiB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 224\n",
        "IMAGE_H = 224\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)\n",
        "num_classes = 7\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "opt_adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "opt_SGD = SGD(learning_rate=0.001)\n",
        "the_arch = 'resnet50'\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model_attention.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/last_model_attention.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_balanced_acc', patience=20, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_balanced_acc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JffFid9sOXeo"
      },
      "outputs": [],
      "source": [
        "# load train and test dataset\n",
        "def preprocess_image_input(input_images, arch = the_arch):\n",
        "  input_images = input_images.astype('float32')\n",
        "  if arch == 'inception_v3':\n",
        "    output_ims = tf.keras.applications.inception_v3.preprocess_input(input_images)\n",
        "  else:\n",
        "    output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def load_cifar10_dataset():\n",
        "  from keras.datasets import cifar10\n",
        "    # load dataset\n",
        "  (X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
        "    # one hot encode target values\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_val = to_categorical(y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def balanced_acc(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "\n",
        "    tensor1 = tf.math.argmax(y_true, axis=1)\n",
        "    tensor2 = tf.math.argmax(y_pred, axis=1)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(tensor1, tensor2)\n",
        "    \n",
        "    diag = tf.linalg.tensor_diag_part (cm)\n",
        "    tpfn = tf.cast(K.sum(cm, axis = 1), tf.float32) + K.epsilon()\n",
        "    recall = tf.divide(tf.cast(diag, tf.float32),tpfn)\n",
        "    balanced_acc = K.mean(recall)\n",
        "    return balanced_acc\n",
        "\n",
        "def define_base_model(arch = the_arch, start_trainable_layer = 9999, attention=False):\n",
        "  #x = UpSampling2D(size=(7,7))(input_tensor)\n",
        "  #x = data_augmentation(input_tensor)\n",
        "  #x = layers.Rescaling(1.0 / 255)(input_tensor)  # Rescale inputs\n",
        "  if arch != 'dense':\n",
        "    input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "    if arch == 'resnet50':\n",
        "      base_model = ResNet50(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'inception_v3':\n",
        "      base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'ResNet':\n",
        "      base_model = ResNet(classes ,image_shape)(input_tensor)\n",
        "    x = base_model.output\n",
        "    if attention:\n",
        "      x = Attention(1024,1024,7,8)(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "    if start_trainable_layer != 9999:\n",
        "      for layer in base_model.layers[start_trainable_layer:]:\n",
        "        layer.trainable = True\n",
        "  else:\n",
        "    input_tensor = Input(shape=(2048))\n",
        "    x = input_tensor\n",
        "  #x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  #x = Dropout(0.2)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  predictions = Dense(num_classes, activation='softmax')(x)\n",
        "  model = Model(inputs=input_tensor, outputs=predictions)\n",
        "  model.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "  return model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "    # plot loss\n",
        "    plt.subplot(211)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    # plot accuracy\n",
        "    plt.subplot(212)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        " \n",
        "# scale pixels\n",
        "def norm_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm\n",
        "\n",
        "def load_isic2018_dataset(train_under_frac = 0):\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "  df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "\n",
        "  #decode one hot label\n",
        "  df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "  df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "  #random undersampling for training dataset\n",
        "  if train_under_frac !=0:\n",
        "    df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=train_under_frac).index)\n",
        "\n",
        "  #drop one-hot column\n",
        "  df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "  df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "  #make filepaths of the image\n",
        "  dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "  dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "  df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "  df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'\n",
        "  \n",
        "  #load image pixels to dataframe\n",
        "  df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "  df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, df_train, df_val\n",
        "\n",
        "def reset_dataset(df_train, df_val):\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  X_train = preprocess_image_input(X_train, the_arch)\n",
        "  X_val = preprocess_image_input(X_val, the_arch)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def SMOTE_Data(X, y, one_hot = False, k = 5, width = IMAGE_W, height = IMAGE_H, c = 3, type = 'smote'):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  if type == 'borderline':\n",
        "    sm = BorderlineSMOTE(random_state=42, k_neighbors=k)\n",
        "  elif type == 'svm':\n",
        "    sm = SVMSMOTE()\n",
        "  elif type == 'adasyn':\n",
        "    sm = ADASYN(random_state=42, n_neighbors=k)\n",
        "  elif type == 'kmeans':\n",
        "    sm = KMeansSMOTE(k_neighbors=k, kmeans_estimator=10)\n",
        "  else:\n",
        "    sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, width * height * c)), y)\n",
        "  X_resampled = X_resampled.reshape(-1, width, height, c)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled\n",
        "\n",
        "def SMOTE_Data2(X, y, one_hot = False, k = 5):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qge6cnxQPnH6",
        "outputId": "58a818eb-cdef-4167-a52e-9eecc1675dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5321, 224, 224, 3)\n",
            "(5321, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 1113, 2: 1099, 1: 514, 0: 327, 6: 142, 3: 115})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "df1 = pd.read_pickle(path+\"isic2018_train.pkl\")\n",
        "X_train = df1.loc[:, df1.columns != 'y_train'].to_numpy()\n",
        "X_train = X_train.reshape(-1,224,224,3)\n",
        "y_train = df1.loc[:, df1.columns == 'y_train'].to_numpy()\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "df1 = pd.read_pickle(path+\"isic2018_val.pkl\")\n",
        "X_val = df1.loc[:, df1.columns != 'y_val'].to_numpy()\n",
        "X_val = X_val.reshape(-1,224,224,3)\n",
        "y_val = df1.loc[:, df1.columns == 'y_val'].to_numpy()\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArGWuciBt_-",
        "outputId": "9f0be5ee-552a-4a49-ec5b-a9b430da6245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14077, 224, 224, 3)\n",
            "(14077, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 2011, 2: 2011, 3: 2011, 0: 2011, 1: 2011, 6: 2011})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7Z_nccu6QjB"
      },
      "outputs": [],
      "source": [
        "#path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "#df1 = pd.DataFrame(X_train.reshape(X_train.shape[0],-1))\n",
        "#df1['y_train'] = np.argmax(y_train, axis=1).tolist()\n",
        "#df2 = pd.DataFrame(X_val.reshape(X_val.shape[0],-1))\n",
        "#df2['y_val'] = np.argmax(y_val, axis=1).tolist()\n",
        "#df1.to_pickle(path+\"isic2018_train.pkl\")\n",
        "#df2.to_pickle(path+\"isic2018_val.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMBgWqIsAAB"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vIygrW81Ln4z",
        "outputId": "6ab8f2dd-f3dc-4023-936b-50a04a64e791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 2s 0us/step\n",
            "94781440/94765736 [==============================] - 2s 0us/step\n",
            "Epoch 1/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.5361 - accuracy: 0.4485 - balanced_acc: 0.4490\n",
            "Epoch 1: val_balanced_acc improved from -inf to 0.23758, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 59s 204ms/step - loss: 1.5361 - accuracy: 0.4485 - balanced_acc: 0.4490 - val_loss: 1.3331 - val_accuracy: 0.5078 - val_balanced_acc: 0.2376 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1475 - accuracy: 0.6097 - balanced_acc: 0.6106\n",
            "Epoch 2: val_balanced_acc improved from 0.23758 to 0.26327, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 42s 188ms/step - loss: 1.1475 - accuracy: 0.6097 - balanced_acc: 0.6106 - val_loss: 1.1995 - val_accuracy: 0.5544 - val_balanced_acc: 0.2633 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0099 - accuracy: 0.6515 - balanced_acc: 0.6507\n",
            "Epoch 3: val_balanced_acc improved from 0.26327 to 0.28071, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 189ms/step - loss: 1.0099 - accuracy: 0.6515 - balanced_acc: 0.6507 - val_loss: 1.0927 - val_accuracy: 0.5803 - val_balanced_acc: 0.2807 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9284 - accuracy: 0.6791 - balanced_acc: 0.6796\n",
            "Epoch 4: val_balanced_acc improved from 0.28071 to 0.34195, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 42s 192ms/step - loss: 0.9284 - accuracy: 0.6791 - balanced_acc: 0.6796 - val_loss: 1.0040 - val_accuracy: 0.6166 - val_balanced_acc: 0.3420 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8674 - accuracy: 0.6971 - balanced_acc: 0.6979\n",
            "Epoch 5: val_balanced_acc improved from 0.34195 to 0.37934, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 42s 191ms/step - loss: 0.8674 - accuracy: 0.6971 - balanced_acc: 0.6979 - val_loss: 1.0038 - val_accuracy: 0.6218 - val_balanced_acc: 0.3793 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8254 - accuracy: 0.7111 - balanced_acc: 0.7084\n",
            "Epoch 6: val_balanced_acc improved from 0.37934 to 0.39482, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 42s 192ms/step - loss: 0.8254 - accuracy: 0.7111 - balanced_acc: 0.7084 - val_loss: 0.9250 - val_accuracy: 0.6632 - val_balanced_acc: 0.3948 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7906 - accuracy: 0.7214 - balanced_acc: 0.7217\n",
            "Epoch 7: val_balanced_acc did not improve from 0.39482\n",
            "219/219 [==============================] - 40s 185ms/step - loss: 0.7906 - accuracy: 0.7214 - balanced_acc: 0.7217 - val_loss: 0.9183 - val_accuracy: 0.6528 - val_balanced_acc: 0.3783 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7577 - accuracy: 0.7372 - balanced_acc: 0.7368\n",
            "Epoch 8: val_balanced_acc improved from 0.39482 to 0.42761, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 42s 194ms/step - loss: 0.7577 - accuracy: 0.7372 - balanced_acc: 0.7368 - val_loss: 0.8455 - val_accuracy: 0.6943 - val_balanced_acc: 0.4276 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7323 - accuracy: 0.7467 - balanced_acc: 0.7446\n",
            "Epoch 9: val_balanced_acc did not improve from 0.42761\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.7323 - accuracy: 0.7467 - balanced_acc: 0.7446 - val_loss: 0.8802 - val_accuracy: 0.6891 - val_balanced_acc: 0.4237 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7085 - accuracy: 0.7547 - balanced_acc: 0.7539\n",
            "Epoch 10: val_balanced_acc improved from 0.42761 to 0.44458, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 42s 193ms/step - loss: 0.7085 - accuracy: 0.7547 - balanced_acc: 0.7539 - val_loss: 0.8729 - val_accuracy: 0.6995 - val_balanced_acc: 0.4446 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6887 - accuracy: 0.7637 - balanced_acc: 0.7659\n",
            "Epoch 11: val_balanced_acc did not improve from 0.44458\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.6887 - accuracy: 0.7637 - balanced_acc: 0.7659 - val_loss: 0.8438 - val_accuracy: 0.7047 - val_balanced_acc: 0.4105 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.7697 - balanced_acc: 0.7709\n",
            "Epoch 12: val_balanced_acc did not improve from 0.44458\n",
            "219/219 [==============================] - 42s 190ms/step - loss: 0.6661 - accuracy: 0.7697 - balanced_acc: 0.7709 - val_loss: 0.8091 - val_accuracy: 0.7358 - val_balanced_acc: 0.4429 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6520 - accuracy: 0.7746 - balanced_acc: 0.7753\n",
            "Epoch 13: val_balanced_acc improved from 0.44458 to 0.44638, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 43s 197ms/step - loss: 0.6520 - accuracy: 0.7746 - balanced_acc: 0.7753 - val_loss: 0.8658 - val_accuracy: 0.6943 - val_balanced_acc: 0.4464 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6322 - accuracy: 0.7811 - balanced_acc: 0.7809\n",
            "Epoch 14: val_balanced_acc improved from 0.44638 to 0.46683, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 43s 194ms/step - loss: 0.6322 - accuracy: 0.7811 - balanced_acc: 0.7809 - val_loss: 0.7699 - val_accuracy: 0.7513 - val_balanced_acc: 0.4668 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.7888 - balanced_acc: 0.7892\n",
            "Epoch 15: val_balanced_acc improved from 0.46683 to 0.47231, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 43s 194ms/step - loss: 0.6188 - accuracy: 0.7888 - balanced_acc: 0.7892 - val_loss: 0.8588 - val_accuracy: 0.7047 - val_balanced_acc: 0.4723 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.6039 - accuracy: 0.7932 - balanced_acc: 0.7928\n",
            "Epoch 16: val_balanced_acc did not improve from 0.47231\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.6039 - accuracy: 0.7932 - balanced_acc: 0.7928 - val_loss: 0.8290 - val_accuracy: 0.7047 - val_balanced_acc: 0.4640 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.7973 - balanced_acc: 0.7951\n",
            "Epoch 17: val_balanced_acc did not improve from 0.47231\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.5933 - accuracy: 0.7973 - balanced_acc: 0.7951 - val_loss: 0.8140 - val_accuracy: 0.7202 - val_balanced_acc: 0.4666 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5768 - accuracy: 0.8050 - balanced_acc: 0.8039\n",
            "Epoch 18: val_balanced_acc did not improve from 0.47231\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.5768 - accuracy: 0.8050 - balanced_acc: 0.8039 - val_loss: 0.8267 - val_accuracy: 0.7098 - val_balanced_acc: 0.4652 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5676 - accuracy: 0.8060 - balanced_acc: 0.8060\n",
            "Epoch 19: val_balanced_acc did not improve from 0.47231\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.5676 - accuracy: 0.8060 - balanced_acc: 0.8060 - val_loss: 0.7755 - val_accuracy: 0.7358 - val_balanced_acc: 0.4492 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.8147 - balanced_acc: 0.8154\n",
            "Epoch 20: val_balanced_acc did not improve from 0.47231\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.5540 - accuracy: 0.8147 - balanced_acc: 0.8154 - val_loss: 0.8051 - val_accuracy: 0.7254 - val_balanced_acc: 0.4678 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5452 - accuracy: 0.8159 - balanced_acc: 0.8166\n",
            "Epoch 21: val_balanced_acc did not improve from 0.47231\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.5452 - accuracy: 0.8159 - balanced_acc: 0.8166 - val_loss: 0.7713 - val_accuracy: 0.7202 - val_balanced_acc: 0.4564 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.8234 - balanced_acc: 0.8229\n",
            "Epoch 22: val_balanced_acc improved from 0.47231 to 0.48192, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 42s 194ms/step - loss: 0.5335 - accuracy: 0.8234 - balanced_acc: 0.8229 - val_loss: 0.7604 - val_accuracy: 0.7668 - val_balanced_acc: 0.4819 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.8261 - balanced_acc: 0.8248\n",
            "Epoch 23: val_balanced_acc did not improve from 0.48192\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.5218 - accuracy: 0.8261 - balanced_acc: 0.8248 - val_loss: 0.8012 - val_accuracy: 0.7306 - val_balanced_acc: 0.4546 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.8267 - balanced_acc: 0.8260\n",
            "Epoch 24: val_balanced_acc did not improve from 0.48192\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.5151 - accuracy: 0.8267 - balanced_acc: 0.8260 - val_loss: 0.7735 - val_accuracy: 0.7358 - val_balanced_acc: 0.4295 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.5055 - accuracy: 0.8339 - balanced_acc: 0.8335\n",
            "Epoch 25: val_balanced_acc did not improve from 0.48192\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.5055 - accuracy: 0.8339 - balanced_acc: 0.8335 - val_loss: 0.7836 - val_accuracy: 0.7306 - val_balanced_acc: 0.4666 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4986 - accuracy: 0.8372 - balanced_acc: 0.8374\n",
            "Epoch 26: val_balanced_acc did not improve from 0.48192\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.4986 - accuracy: 0.8372 - balanced_acc: 0.8374 - val_loss: 0.8320 - val_accuracy: 0.6891 - val_balanced_acc: 0.4164 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4871 - accuracy: 0.8386 - balanced_acc: 0.8381\n",
            "Epoch 27: val_balanced_acc did not improve from 0.48192\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.4871 - accuracy: 0.8386 - balanced_acc: 0.8381 - val_loss: 0.7984 - val_accuracy: 0.7254 - val_balanced_acc: 0.4428 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4812 - accuracy: 0.8434 - balanced_acc: 0.8436\n",
            "Epoch 28: val_balanced_acc improved from 0.48192 to 0.50140, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 43s 194ms/step - loss: 0.4812 - accuracy: 0.8434 - balanced_acc: 0.8436 - val_loss: 0.7984 - val_accuracy: 0.7358 - val_balanced_acc: 0.5014 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4731 - accuracy: 0.8460 - balanced_acc: 0.8441\n",
            "Epoch 29: val_balanced_acc did not improve from 0.50140\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.4731 - accuracy: 0.8460 - balanced_acc: 0.8441 - val_loss: 0.7393 - val_accuracy: 0.7409 - val_balanced_acc: 0.4429 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4639 - accuracy: 0.8482 - balanced_acc: 0.8504\n",
            "Epoch 30: val_balanced_acc did not improve from 0.50140\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.4639 - accuracy: 0.8482 - balanced_acc: 0.8504 - val_loss: 0.7742 - val_accuracy: 0.7306 - val_balanced_acc: 0.4489 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4583 - accuracy: 0.8524 - balanced_acc: 0.8529\n",
            "Epoch 31: val_balanced_acc did not improve from 0.50140\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.4583 - accuracy: 0.8524 - balanced_acc: 0.8529 - val_loss: 0.7823 - val_accuracy: 0.7150 - val_balanced_acc: 0.4383 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.8555 - balanced_acc: 0.8576\n",
            "Epoch 32: val_balanced_acc did not improve from 0.50140\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.4494 - accuracy: 0.8555 - balanced_acc: 0.8576 - val_loss: 0.7338 - val_accuracy: 0.7409 - val_balanced_acc: 0.4597 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4432 - accuracy: 0.8566 - balanced_acc: 0.8575\n",
            "Epoch 33: val_balanced_acc did not improve from 0.50140\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.4432 - accuracy: 0.8566 - balanced_acc: 0.8575 - val_loss: 0.7918 - val_accuracy: 0.7098 - val_balanced_acc: 0.4360 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4399 - accuracy: 0.8591 - balanced_acc: 0.8589\n",
            "Epoch 34: val_balanced_acc did not improve from 0.50140\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.4399 - accuracy: 0.8591 - balanced_acc: 0.8589 - val_loss: 0.7629 - val_accuracy: 0.7150 - val_balanced_acc: 0.4449 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.8614 - balanced_acc: 0.8631\n",
            "Epoch 35: val_balanced_acc did not improve from 0.50140\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.4303 - accuracy: 0.8614 - balanced_acc: 0.8631 - val_loss: 0.7408 - val_accuracy: 0.7202 - val_balanced_acc: 0.4112 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.8640 - balanced_acc: 0.8644\n",
            "Epoch 36: val_balanced_acc did not improve from 0.50140\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.4245 - accuracy: 0.8640 - balanced_acc: 0.8644 - val_loss: 0.7803 - val_accuracy: 0.7306 - val_balanced_acc: 0.4523 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.8648 - balanced_acc: 0.8654\n",
            "Epoch 37: val_balanced_acc improved from 0.50140 to 0.51077, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 43s 194ms/step - loss: 0.4189 - accuracy: 0.8648 - balanced_acc: 0.8654 - val_loss: 0.7598 - val_accuracy: 0.7358 - val_balanced_acc: 0.5108 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8699 - balanced_acc: 0.8692\n",
            "Epoch 38: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.4118 - accuracy: 0.8699 - balanced_acc: 0.8692 - val_loss: 0.7077 - val_accuracy: 0.7617 - val_balanced_acc: 0.5062 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4037 - accuracy: 0.8706 - balanced_acc: 0.8699\n",
            "Epoch 39: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.4037 - accuracy: 0.8706 - balanced_acc: 0.8699 - val_loss: 0.7057 - val_accuracy: 0.7461 - val_balanced_acc: 0.4452 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.8724 - balanced_acc: 0.8725\n",
            "Epoch 40: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.4015 - accuracy: 0.8724 - balanced_acc: 0.8725 - val_loss: 0.7942 - val_accuracy: 0.7150 - val_balanced_acc: 0.4534 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.8768 - balanced_acc: 0.8774\n",
            "Epoch 41: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3952 - accuracy: 0.8768 - balanced_acc: 0.8774 - val_loss: 0.7339 - val_accuracy: 0.7254 - val_balanced_acc: 0.4052 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3899 - accuracy: 0.8775 - balanced_acc: 0.8762\n",
            "Epoch 42: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3899 - accuracy: 0.8775 - balanced_acc: 0.8762 - val_loss: 0.7390 - val_accuracy: 0.7254 - val_balanced_acc: 0.4160 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3822 - accuracy: 0.8812 - balanced_acc: 0.8807\n",
            "Epoch 43: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.3822 - accuracy: 0.8812 - balanced_acc: 0.8807 - val_loss: 0.7258 - val_accuracy: 0.7461 - val_balanced_acc: 0.4629 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3784 - accuracy: 0.8813 - balanced_acc: 0.8808\n",
            "Epoch 44: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.3784 - accuracy: 0.8813 - balanced_acc: 0.8808 - val_loss: 0.7439 - val_accuracy: 0.7150 - val_balanced_acc: 0.4185 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3740 - accuracy: 0.8833 - balanced_acc: 0.8837\n",
            "Epoch 45: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.3740 - accuracy: 0.8833 - balanced_acc: 0.8837 - val_loss: 0.7291 - val_accuracy: 0.7358 - val_balanced_acc: 0.4433 - lr: 0.0010\n",
            "Epoch 46/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3682 - accuracy: 0.8869 - balanced_acc: 0.8875\n",
            "Epoch 46: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3682 - accuracy: 0.8869 - balanced_acc: 0.8875 - val_loss: 0.7512 - val_accuracy: 0.7254 - val_balanced_acc: 0.4292 - lr: 0.0010\n",
            "Epoch 47/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8839 - balanced_acc: 0.8839\n",
            "Epoch 47: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3678 - accuracy: 0.8839 - balanced_acc: 0.8839 - val_loss: 0.7563 - val_accuracy: 0.7098 - val_balanced_acc: 0.4169 - lr: 0.0010\n",
            "Epoch 48/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3575 - accuracy: 0.8905 - balanced_acc: 0.8914\n",
            "Epoch 48: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 188ms/step - loss: 0.3575 - accuracy: 0.8905 - balanced_acc: 0.8914 - val_loss: 0.7526 - val_accuracy: 0.7150 - val_balanced_acc: 0.4170 - lr: 0.0010\n",
            "Epoch 49/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3537 - accuracy: 0.8900 - balanced_acc: 0.8896\n",
            "Epoch 49: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3537 - accuracy: 0.8900 - balanced_acc: 0.8896 - val_loss: 0.7222 - val_accuracy: 0.7513 - val_balanced_acc: 0.4557 - lr: 0.0010\n",
            "Epoch 50/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3540 - accuracy: 0.8895 - balanced_acc: 0.8915\n",
            "Epoch 50: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3540 - accuracy: 0.8895 - balanced_acc: 0.8915 - val_loss: 0.7646 - val_accuracy: 0.7150 - val_balanced_acc: 0.4296 - lr: 0.0010\n",
            "Epoch 51/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3486 - accuracy: 0.8934 - balanced_acc: 0.8921\n",
            "Epoch 51: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3486 - accuracy: 0.8934 - balanced_acc: 0.8921 - val_loss: 0.7132 - val_accuracy: 0.7565 - val_balanced_acc: 0.4146 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.8970 - balanced_acc: 0.8983\n",
            "Epoch 52: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.3383 - accuracy: 0.8970 - balanced_acc: 0.8983 - val_loss: 0.7316 - val_accuracy: 0.7306 - val_balanced_acc: 0.4202 - lr: 0.0010\n",
            "Epoch 53/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3367 - accuracy: 0.8957 - balanced_acc: 0.8965\n",
            "Epoch 53: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.3367 - accuracy: 0.8957 - balanced_acc: 0.8965 - val_loss: 0.7541 - val_accuracy: 0.7202 - val_balanced_acc: 0.4460 - lr: 0.0010\n",
            "Epoch 54/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.8985 - balanced_acc: 0.8979\n",
            "Epoch 54: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3348 - accuracy: 0.8985 - balanced_acc: 0.8979 - val_loss: 0.7155 - val_accuracy: 0.7358 - val_balanced_acc: 0.4289 - lr: 0.0010\n",
            "Epoch 55/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8997 - balanced_acc: 0.9008\n",
            "Epoch 55: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 42s 190ms/step - loss: 0.3303 - accuracy: 0.8997 - balanced_acc: 0.9008 - val_loss: 0.7416 - val_accuracy: 0.7306 - val_balanced_acc: 0.4220 - lr: 0.0010\n",
            "Epoch 56/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3225 - accuracy: 0.9012 - balanced_acc: 0.9001\n",
            "Epoch 56: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 42s 190ms/step - loss: 0.3225 - accuracy: 0.9012 - balanced_acc: 0.9001 - val_loss: 0.7082 - val_accuracy: 0.7617 - val_balanced_acc: 0.4478 - lr: 0.0010\n",
            "Epoch 57/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.9034 - balanced_acc: 0.9034\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 57: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 188ms/step - loss: 0.3237 - accuracy: 0.9034 - balanced_acc: 0.9034 - val_loss: 0.7484 - val_accuracy: 0.7202 - val_balanced_acc: 0.4221 - lr: 0.0010\n",
            "Epoch 58/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.9058 - balanced_acc: 0.9070\n",
            "Epoch 58: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 188ms/step - loss: 0.3146 - accuracy: 0.9058 - balanced_acc: 0.9070 - val_loss: 0.7124 - val_accuracy: 0.7565 - val_balanced_acc: 0.4494 - lr: 5.0000e-04\n",
            "Epoch 59/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3142 - accuracy: 0.9052 - balanced_acc: 0.9059\n",
            "Epoch 59: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3142 - accuracy: 0.9052 - balanced_acc: 0.9059 - val_loss: 0.7352 - val_accuracy: 0.7358 - val_balanced_acc: 0.4488 - lr: 5.0000e-04\n",
            "Epoch 60/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.9106 - balanced_acc: 0.9088\n",
            "Epoch 60: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3081 - accuracy: 0.9106 - balanced_acc: 0.9088 - val_loss: 0.7248 - val_accuracy: 0.7513 - val_balanced_acc: 0.4519 - lr: 5.0000e-04\n",
            "Epoch 61/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.9099 - balanced_acc: 0.9108\n",
            "Epoch 61: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3084 - accuracy: 0.9099 - balanced_acc: 0.9108 - val_loss: 0.7169 - val_accuracy: 0.7565 - val_balanced_acc: 0.4253 - lr: 5.0000e-04\n",
            "Epoch 62/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3070 - accuracy: 0.9097 - balanced_acc: 0.9085\n",
            "Epoch 62: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.3070 - accuracy: 0.9097 - balanced_acc: 0.9085 - val_loss: 0.7341 - val_accuracy: 0.7306 - val_balanced_acc: 0.4477 - lr: 5.0000e-04\n",
            "Epoch 63/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.9116 - balanced_acc: 0.9109\n",
            "Epoch 63: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.3053 - accuracy: 0.9116 - balanced_acc: 0.9109 - val_loss: 0.7136 - val_accuracy: 0.7513 - val_balanced_acc: 0.4521 - lr: 5.0000e-04\n",
            "Epoch 64/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.9128 - balanced_acc: 0.9127\n",
            "Epoch 64: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.3027 - accuracy: 0.9128 - balanced_acc: 0.9127 - val_loss: 0.7514 - val_accuracy: 0.7306 - val_balanced_acc: 0.4238 - lr: 5.0000e-04\n",
            "Epoch 65/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.9110 - balanced_acc: 0.9116\n",
            "Epoch 65: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.3030 - accuracy: 0.9110 - balanced_acc: 0.9116 - val_loss: 0.7148 - val_accuracy: 0.7513 - val_balanced_acc: 0.4527 - lr: 5.0000e-04\n",
            "Epoch 66/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.9102 - balanced_acc: 0.9109\n",
            "Epoch 66: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.2999 - accuracy: 0.9102 - balanced_acc: 0.9109 - val_loss: 0.7159 - val_accuracy: 0.7461 - val_balanced_acc: 0.4450 - lr: 5.0000e-04\n",
            "Epoch 67/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.9142 - balanced_acc: 0.9140\n",
            "Epoch 67: val_balanced_acc did not improve from 0.51077\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.3011 - accuracy: 0.9142 - balanced_acc: 0.9140 - val_loss: 0.7173 - val_accuracy: 0.7409 - val_balanced_acc: 0.4528 - lr: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# define model\n",
        "model = define_base_model('resnet50')\n",
        "#model.summary()\n",
        "hst = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n",
        "# learning curves\n",
        "summarize_diagnostics(hst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vXnW3lmCgln3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "50836a5a-dc5e-49aa-b4de-41083786cf07"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwwLiXUSG0IZ"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "#hst = model.fit(train_data_batches,\n",
        "#                    epochs = EPOCHS, validation_data = valid_data_batches,      \n",
        "                    #steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "#                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgjmi-4UIT-"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SPz8NH1Oylv9"
      },
      "outputs": [],
      "source": [
        "#save last model\n",
        "model.save(last_model_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS3ewyxO_anU",
        "outputId": "7b2dbb2d-4cb1-4b3e-c95a-6ee2ca2d2c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.9166725864885984\n",
            "balanced accuracy on training 0.9166725864885984\n",
            "accuracy on validation 0.7409326424870466\n",
            "balanced accuracy on validation 0.5790307253721888\n",
            "Score on val data:  (0.523556459739224, 0.5790307253721888, 0.5442752572727204, None)\n"
          ]
        }
      ],
      "source": [
        "last_model = load_model(last_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = last_model.predict(X_train)\n",
        "y_val_pred = last_model.predict(X_val)\n",
        "\n",
        "#print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3IyWjdGG4Xq",
        "outputId": "77604758-2733-4570-9937-2c2de8de2353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.8704269375577183\n",
            "balanced accuracy on training 0.8704269375577182\n",
            "accuracy on validation 0.7357512953367875\n",
            "balanced accuracy on validation 0.7308765856675264\n",
            "Score on val data:  (0.5736181670392196, 0.7308765856675264, 0.6051913812367793, None)\n"
          ]
        }
      ],
      "source": [
        "best_model = load_model(best_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRWiTnO0MGh"
      },
      "source": [
        "#Cut-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGnCoIdLyDHS"
      },
      "outputs": [],
      "source": [
        "df_val_pred = pd.DataFrame(y_val_pred, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdyCbloQyWTC"
      },
      "outputs": [],
      "source": [
        "numbers = [float(x)/40 for x in range(11)]\n",
        "for i in numbers:\n",
        "    df_val_pred[i]= df_val_pred.MEL.map(lambda x: 1 if x > i else 0)\n",
        "df_val_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4SQsRx73kgk"
      },
      "outputs": [],
      "source": [
        "y_val_true= [1 if x == 4 else 0 for x in np.argmax(y_val, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcUISWFi0J05"
      },
      "outputs": [],
      "source": [
        "#num = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
        "cutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n",
        "for i in numbers:\n",
        "    cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "    total1=sum(sum(cm1))\n",
        "    Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31LSzov1tCt"
      },
      "outputs": [],
      "source": [
        "cutoff_df[['Accuracy','Sensitivity','Specificity']].plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CIKT94Jqye"
      },
      "outputs": [],
      "source": [
        "i = 0.025\n",
        "cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "total1=sum(sum(cm1))\n",
        "Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U2tkFebL_VC"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ', Accuracy)\n",
        "print('Sensitivity: ', Sensitivity)\n",
        "print('Specificity: ', Specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkPOFLehOmFg"
      },
      "outputs": [],
      "source": [
        "#change melanoma flag back to 4\n",
        "df_val_pred[df_val_pred[i] == 1] = 4\n",
        "#decode one-hot y_val_pred while use cut-off melanoma data\n",
        "condition = df_val_pred[i] == 4\n",
        "y_val_pred2 = np.where(condition, df_val_pred[i], np.argmax(y_val_pred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOVl6dWlTDLo"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ',accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))\n",
        "print('Balanced accuracy: ',balanced_accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvYutTKRhR_"
      },
      "outputs": [],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_val, axis=1), y_val_pred2)\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtvW3YeaLlC"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-1yjWGeKs7"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "#labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "#f = plt.figure(figsize=(15, 6))\n",
        "#s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "#s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic95mefkpG3"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "df_test['image_px'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60LYAT7VsNOZ"
      },
      "outputs": [],
      "source": [
        "X_test = np.asarray(df_test['image_px'].tolist())\n",
        "print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXnnIIwC4cHE"
      },
      "outputs": [],
      "source": [
        "#preprocess\n",
        "X_test = preprocess_image_input(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeDTXdaMLmyU"
      },
      "outputs": [],
      "source": [
        "#X_test = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIX0AmEFNv3Y"
      },
      "outputs": [],
      "source": [
        "# predicting\n",
        "#CHANGE THE MODEL IF NECESSARY\n",
        "#X_test2 = model1.predict(X_test)\n",
        "Y_pred2 = model.predict(X_test2)\n",
        "print(\"Y_pred2\", Y_pred2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]\n",
        "df_pred.set_index(\"image\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ynyd8PjT589"
      },
      "outputs": [],
      "source": [
        "#update MEL data using cut-off value\n",
        "df_pred.MEL[df_pred.MEL > i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjRdONoQVMq0"
      },
      "outputs": [],
      "source": [
        "df_pred.loc[df_pred.MEL > i, ['NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_SMOTE_Attention.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0MghVs0tsGw"
      },
      "source": [
        "result: 0.656"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswA0co2y1wl"
      },
      "source": [
        "#Exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnqJYIONy34l"
      },
      "outputs": [],
      "source": [
        "input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "base_model = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model(input_tensor, training=False)\n",
        "x = Attention(2048,2048,7,8)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "res50 = Model(inputs=input_tensor, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcn8hQg3J8yP"
      },
      "outputs": [],
      "source": [
        "#Train i-last layer\n",
        "# summarize feature map shapes\n",
        "for i in range(len(model.layers)):\n",
        "    layer = model.layers[i]\n",
        "    # summarize output shape\n",
        "    print(i, layer.name, layer.output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA7Af2Y73FUv"
      },
      "outputs": [],
      "source": [
        "X_train = res50.predict(X_train)\n",
        "X_val = res50.predict(X_val)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krJiAb1m3QNf"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = SMOTE_Data2(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfcFpsBwM0d4"
      },
      "source": [
        "#Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_s6OIGKM26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "371bd24a-4bf9-491a-e0ec-78b7eb06e6d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ff488085aaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_current_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_preprocessing_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Image preprocessing layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m \u001b[0mcython_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'core'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tk\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,AveragePooling2D,BatchNormalization,Add,ZeroPadding2D,Flatten,Dense,Input,LeakyReLU,Softmax,ReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class Attention(tk.layers.Layer):\n",
        "    \n",
        "    def __init__(self,input_channels,output_channel,kernel_size,groups):\n",
        "        super().__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channel = output_channel    \n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = 1\n",
        "        self.groups = groups\n",
        "\n",
        "        assert output_channel % groups == 0\n",
        "        \n",
        "        self.rel_h = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,kernel_size,1,output_channel//2),stddev = 0.1)) \n",
        "        #output_channels//2 is the number of channels on which the relative position will be considered,1 denotes the number of those filters and the one after that too and (kernel_size,1) denotes the size of that filter\n",
        "        self.rel_w = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,1,kernel_size,output_channel//2),stddev = 0.1)) \n",
        "\n",
        "        self.key_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.query_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.value_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "\n",
        "    def call(self,x):\n",
        "        \n",
        "        batch,height,width,channels = x.shape\n",
        "        x_padded = ZeroPadding2D(padding=(self.kernel_size//2,self.kernel_size//2))(x)\n",
        "        query = self.query_weights(x)\n",
        "        value = self.value_weights(x_padded)\n",
        "        key = self.key_weights(x_padded)\n",
        "        #key,query and value will have the shape of (batch,height,width,depth)\n",
        "        keys = tf.image.extract_patches(images = key,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        value = tf.image.extract_patches(images = value,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        no_of_kernels = key.shape[-2] - self.kernel_size + 1\n",
        "        keys = tf.reshape(keys,shape = (-1,no_of_kernels, no_of_kernels,self.kernel_size,self.kernel_size,self.output_channel))\n",
        "        key_split_h,key_split_w = tf.split(keys,num_or_size_splits = 2,axis = -1)\n",
        "        key_with_rel = tk.layers.concatenate([key_split_h + self.rel_h,key_split_w + self.rel_w],axis = -1) \n",
        "        \n",
        "        #reshaping the query and key\n",
        "        key_with_rel = tf.reshape(key_with_rel,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        query  = tf.reshape(query,(-1,self.groups,no_of_kernels,no_of_kernels,1,self.output_channel//self.groups))        \n",
        "        value = tf.reshape(value,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        \n",
        "        #multiplication  of key and query\n",
        "        #assert key_with_rel.shape == query.shape        \n",
        "        key_prod_query = query*key_with_rel\n",
        "        \n",
        "        # Now the function is passed through the softmax and is multiplied with the values\n",
        "        s = Softmax(axis = -2)(key_prod_query)\n",
        "        y = tf.einsum('bnchwk,bnchwk->bnchk',s,value)\n",
        "        y = tf.reshape(y,(-1,height,width,self.output_channel))\n",
        "        return y\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            \"input_channels\": self.input_channels, \n",
        "            \"output_channel\": self.output_channel, \n",
        "            \"kernel_size\": self.kernel_size, \n",
        "            \"stride\": self.stride, \n",
        "            \"groups\": self.groups, \n",
        "            \"rel_h\": self.rel_h, \n",
        "            \"rel_w\": self.rel_w, \n",
        "            \"key_weights\": self.key_weights, \n",
        "            \"query_weights\": self.query_weights, \n",
        "            \"value_weights\": self.value_weights\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE8Ziq-BlEP4"
      },
      "source": [
        "#Oversampling on feature map level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtgmvyhCndpB"
      },
      "outputs": [],
      "source": [
        "i = 176"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm05Zet_B5am"
      },
      "outputs": [],
      "source": [
        "for i in range(len(model.layers)):\n",
        "  layer = model.layers[i]\n",
        "  print(i, layer.name, layer.output_shape, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqeSic6NmLsR"
      },
      "outputs": [],
      "source": [
        "# redefine model to output right after the first hidden layer\n",
        "model1 = Model(inputs=model.inputs, outputs=model.layers[i].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVHYG9Rwm28i"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "X_train_fm = model1.predict(X_train)\n",
        "X_val_fm = model1.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNozN8-wDUNL"
      },
      "outputs": [],
      "source": [
        "X_train_fm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19hK7aQNeAQo"
      },
      "outputs": [],
      "source": [
        "X_train_fm_ov, y_train_ov = SMOTE_Data2(X_train_fm, y_train, True, 5)\n",
        "print(X_train_fm_ov.shape)\n",
        "print(y_train_ov.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train_ov, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qP4iyYcnAYa"
      },
      "outputs": [],
      "source": [
        "model2 = Model(inputs=model.layers[i].output, outputs=model.layers[len(model.layers)-1].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzdjs0WbvDB0"
      },
      "outputs": [],
      "source": [
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/last_model_no.h5'\n",
        "model2.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst = model2.fit(X_train_fm_ov, y_train_ov, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val_fm, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train_fm_ov.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XhlbWn--8Or"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW-_U6vFpIci"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "y_train_pred = model2.predict(X_train_fm_ov)\n",
        "y_val_pred = model2.predict(X_val_fm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLop0YK-ZK40"
      },
      "outputs": [],
      "source": [
        "print('accuracy on training',accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load ISIC 2018 Challange Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3P7IjyLuZGY"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_isic2018_dataset(train_under_frac = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IncA-_o_n5w"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = y_train.value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(y_train == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "labels = sorted(df1['y_train'].unique())\n",
        "for label in range(7):\n",
        "    plot_images_per_label(df1, label, 3, (12,9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asV1O58Lrq-R"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.fromarray(X_train[0], 'RGB')\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "#df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "#df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "#df_train = df_train.drop_duplicates(subset=['lesion_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmX_Uqbmj-tN"
      },
      "outputs": [],
      "source": [
        "from numpy import moveaxis\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_el = np.inf\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 300       # how many epochs to run for\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "args['patience'] = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NydOdPMajEfT"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        # [(WK+2P)/S]+1\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(7, stride=7),\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),# 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False), # 8\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),# 4\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 0, bias=False),#14\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**3 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        # H_out = (H_in1)*stride[0]  2padding[0] + dilation[0](kernel_size[0]1) + output_padding[0] + 1\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4), #10\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4), #13\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h, 4),# 16\n",
        "            nn.BatchNorm2d(self.dim_h),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h, 3, 4, 2, 1),# 32\n",
        "            nn.UpsamplingBilinear2d(scale_factor=7),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**3, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def DeepSMOTE_train(X_train, y_train, one_hot = False):\n",
        "  from torch.utils.data import TensorDataset\n",
        "  import os\n",
        "\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #X_train = X_train.astype('float32') / 255.\n",
        "  \n",
        "  batch_size = args['batch_size']\n",
        "  patience = args['patience']\n",
        "  encoder = Encoder(args)\n",
        "  decoder = Decoder(args)\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  print(device)\n",
        "  decoder = decoder.to(device)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "  #decoder loss function\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  num_workers = 0\n",
        "\n",
        "  #torch.Tensor returns float so if want long then use torch.tensor\n",
        "  tensor_x = torch.from_numpy(X_train.copy())#torch.Tensor(X_train)\n",
        "  tensor_y = torch.tensor(y_train,dtype=torch.long)\n",
        "  mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "  train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "      batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "  best_loss = np.inf\n",
        "\n",
        "  enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "  dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "  for epoch in range(args['epochs']):\n",
        "      train_loss = 0.0\n",
        "      tmse_loss = 0.0\n",
        "      tdiscr_loss = 0.0\n",
        "      # train for one epoch -- set nets to train mode\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "  \n",
        "      for images,labs in train_loader:\n",
        "      \n",
        "          # zero gradients for each batch\n",
        "          encoder.zero_grad()\n",
        "          decoder.zero_grad()\n",
        "          images, labs = images.to(device), labs.to(device)\n",
        "          labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "          # run images\n",
        "          z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "      \n",
        "          x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "          mse = criterion(x_hat,images)\n",
        "                  \n",
        "          resx = []\n",
        "          resy = []\n",
        "      \n",
        "          tc = np.random.choice(num_classes,1)\n",
        "          #tc = 9\n",
        "          xbeg = X_train[y_train == tc]\n",
        "          ybeg = y_train[y_train == tc] \n",
        "          xlen = len(xbeg)\n",
        "          nsamp = min(xlen, 100)\n",
        "          ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "          xclass = xbeg[ind]\n",
        "          yclass = ybeg[ind]\n",
        "      \n",
        "          xclen = len(xclass)\n",
        "          xcminus = np.arange(1,xclen)\n",
        "          \n",
        "          xcplus = np.append(xcminus,0)\n",
        "          xcnew = (xclass[[xcplus],:])\n",
        "          xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "      \n",
        "          xcnew = torch.Tensor(xcnew)\n",
        "          xcnew = xcnew.to(device)\n",
        "      \n",
        "          #encode xclass to feature space\n",
        "          xclass = torch.Tensor(xclass)\n",
        "          xclass = xclass.to(device)\n",
        "          xclass = encoder(xclass)\n",
        "      \n",
        "          xclass = xclass.detach().cpu().numpy()\n",
        "      \n",
        "          xc_enc = (xclass[[xcplus],:])\n",
        "          xc_enc = np.squeeze(xc_enc)\n",
        "      \n",
        "          xc_enc = torch.Tensor(xc_enc)\n",
        "          xc_enc = xc_enc.to(device)\n",
        "          \n",
        "          ximg = decoder(xc_enc)\n",
        "          \n",
        "          mse2 = criterion(ximg,xcnew)\n",
        "      \n",
        "          comb_loss = mse2 + mse\n",
        "          comb_loss.backward()\n",
        "      \n",
        "          enc_optim.step()\n",
        "          dec_optim.step()\n",
        "      \n",
        "          train_loss += comb_loss.item()*images.size(0)\n",
        "          tmse_loss += mse.item()*images.size(0)\n",
        "          tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "      train_loss = train_loss/len(train_loader)\n",
        "      tmse_loss = tmse_loss/len(train_loader)\n",
        "      tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "      print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "              train_loss,tmse_loss,tdiscr_loss))\n",
        "      \n",
        "  \n",
        "  \n",
        "      #store the best encoder and decoder models\n",
        "      #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "      #necessary for illustration purposes\n",
        "      if train_loss < best_loss:\n",
        "          print('Saving..')\n",
        "          patience = args['patience']\n",
        "          path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_enc.pth'\n",
        "          path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_dec.pth'\n",
        "        \n",
        "          torch.save(encoder.state_dict(), path_enc)\n",
        "          torch.save(decoder.state_dict(), path_dec)\n",
        "  \n",
        "          best_loss = train_loss\n",
        "      else:\n",
        "          patience = patience - 1\n",
        "\n",
        "      if patience == 0:\n",
        "          print('Out of patience. \\n')\n",
        "          break\n",
        "\n",
        "def DeepSMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  batch_size = args['batch_size']\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #Generate artificial images\n",
        "  import torch\n",
        "  np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "  #path on the computer where the models are stored\n",
        "  modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/'\n",
        "\n",
        "  path_enc = modpth + '/bst_enc.pth'\n",
        "  path_dec = modpth + '/bst_dec.pth'\n",
        "  \n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  encoder = Encoder(args)\n",
        "  encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  decoder = Decoder(args)\n",
        "  decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "  decoder = decoder.to(device)\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  resx = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      #encode xclass to feature space\n",
        "      xclass = torch.Tensor(xclass)\n",
        "      xclass = xclass.to(device)\n",
        "      xclass = encoder(xclass)\n",
        "          \n",
        "      xclass = xclass.detach().cpu().numpy()\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n == 0:\n",
        "        continue\n",
        "#        resx2 = []\n",
        "#        resy2 = []\n",
        "#        for j in range(batch_size, n+batch_size+1, batch_size):\n",
        "#          if j <= n:\n",
        "#            batch_size_max = batch_size\n",
        "#          elif n % batch_size != 0:\n",
        "#            batch_size_max = n%batch_size\n",
        "#          else:\n",
        "#            break\n",
        "#          xsamp, ysamp = G_SM(xclass,yclass,batch_size_max,i)\n",
        "      xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "  \n",
        "      \"\"\"to generate samples for resnet\"\"\"   \n",
        "      xsamp = torch.Tensor(xsamp)\n",
        "      xsamp = xsamp.to(device)\n",
        "      ximg = decoder(xsamp)\n",
        "\n",
        "      ximn = ximg.detach().cpu().numpy()\n",
        "#        resx2.append(ximn)\n",
        "#        resy2.append(ysamp)\n",
        "#        \n",
        "#        resx2 = np.vstack(resx2)\n",
        "#        resy2 = np.hstack(resy2)\n",
        "      resx.append(ximn)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx1 = np.vstack(resx)\n",
        "  resy1 = np.hstack(resy)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = X_train.reshape(X_train.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "  X_train = moveaxis(X_train, 1, 3)\n",
        "  X_train = X_train * max_el\n",
        "  return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6qneWL_Bs2U"
      },
      "outputs": [],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr1jnSM7yzJc"
      },
      "outputs": [],
      "source": [
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1aAQBmiy0K"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['balanced_acc'])\n",
        "plt.plot(hst2.history['val_balanced_acc'])\n",
        "plt.title('model balance_acc after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UswA0co2y1wl",
        "iDRWiTnO0MGh",
        "eaK4zbtoaAaC",
        "3K908bbiYwbS",
        "kE8Ziq-BlEP4",
        "RcRGeofw-8tK",
        "cNBXx28B9yGu",
        "0jrJ33lUDkCM",
        "B2PgksTFkOAq"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}