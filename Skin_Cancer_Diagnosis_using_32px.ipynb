{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Size-Comparison/blob/main/Skin_Cancer_Diagnosis_using_32px.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "26aec09c-09a9-4d26-c381-342eac0e42f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_x4c0_DTkaa"
      },
      "source": [
        "#Library, atribut, and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nR2MJBYq-oiB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 32\n",
        "IMAGE_H = 32\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)\n",
        "num_classes = 7\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "opt_adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "opt_SGD = SGD(learning_rate=0.001)\n",
        "the_arch = 'resnet50'\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model_attention.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/last_model_attention.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_balanced_acc', patience=20, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_balanced_acc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JffFid9sOXeo"
      },
      "outputs": [],
      "source": [
        "# load train and test dataset\n",
        "def preprocess_image_input(input_images, arch = the_arch):\n",
        "  input_images = input_images.astype('float32')\n",
        "  if arch == 'inception_v3':\n",
        "    output_ims = tf.keras.applications.inception_v3.preprocess_input(input_images)\n",
        "  else:\n",
        "    output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def load_cifar10_dataset():\n",
        "  from keras.datasets import cifar10\n",
        "    # load dataset\n",
        "  (X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
        "    # one hot encode target values\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_val = to_categorical(y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def balanced_acc(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "\n",
        "    tensor1 = tf.math.argmax(y_true, axis=1)\n",
        "    tensor2 = tf.math.argmax(y_pred, axis=1)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(tensor1, tensor2)\n",
        "    \n",
        "    diag = tf.linalg.tensor_diag_part (cm)\n",
        "    tpfn = tf.cast(K.sum(cm, axis = 1), tf.float32) + K.epsilon()\n",
        "    recall = tf.divide(tf.cast(diag, tf.float32),tpfn)\n",
        "    balanced_acc = K.mean(recall)\n",
        "    return balanced_acc\n",
        "\n",
        "def define_base_model(arch = the_arch, start_trainable_layer = 9999, attention=False):\n",
        "  #x = data_augmentation(input_tensor)\n",
        "  #x = layers.Rescaling(1.0 / 255)(input_tensor)  # Rescale inputs\n",
        "  if arch != 'dense':\n",
        "    input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "    if IMAGE_H == 32:\n",
        "      x = UpSampling2D(size=(7,7))(input_tensor)\n",
        "    if arch == 'resnet50':\n",
        "      base_model = ResNet50(input_tensor=x, weights='imagenet', include_top=False)\n",
        "    elif arch == 'inception_v3':\n",
        "      base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "    elif arch == 'ResNet':\n",
        "      base_model = ResNet(classes ,image_shape)(input_tensor)\n",
        "    x = base_model.output\n",
        "    if attention:\n",
        "      x = Attention(1024,1024,7,8)(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "    if start_trainable_layer != 9999:\n",
        "      for layer in base_model.layers[start_trainable_layer:]:\n",
        "        layer.trainable = True\n",
        "  else:\n",
        "    input_tensor = Input(shape=(2048))\n",
        "    x = input_tensor\n",
        "  #x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  #x = Dropout(0.2)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  predictions = Dense(num_classes, activation='softmax')(x)\n",
        "  model = Model(inputs=input_tensor, outputs=predictions)\n",
        "  model.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "  return model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "    # plot loss\n",
        "    plt.subplot(211)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    # plot accuracy\n",
        "    plt.subplot(212)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        " \n",
        "# scale pixels\n",
        "def norm_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm\n",
        "\n",
        "def load_isic2018_dataset(train_under_frac = 0):\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "  df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "\n",
        "  #decode one hot label\n",
        "  df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "  df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "  #random undersampling for training dataset\n",
        "  if train_under_frac !=0:\n",
        "    df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=train_under_frac).index)\n",
        "\n",
        "  #drop one-hot column\n",
        "  df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "  df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "  #make filepaths of the image\n",
        "  dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "  dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "  df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "  df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'\n",
        "  \n",
        "  #load image pixels to dataframe\n",
        "  df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "  df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def reset_dataset(df_train, df_val):\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  X_train = preprocess_image_input(X_train, the_arch)\n",
        "  X_val = preprocess_image_input(X_val, the_arch)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def SMOTE_Data(X, y, one_hot = False, k = 5, width = IMAGE_W, height = IMAGE_H, c = 3, type = 'smote'):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  if type == 'borderline':\n",
        "    sm = BorderlineSMOTE(random_state=42, k_neighbors=k)\n",
        "  elif type == 'svm':\n",
        "    sm = SVMSMOTE()\n",
        "  elif type == 'adasyn':\n",
        "    sm = ADASYN(random_state=42, n_neighbors=k)\n",
        "  elif type == 'kmeans':\n",
        "    sm = KMeansSMOTE(k_neighbors=k, kmeans_estimator=10)\n",
        "  else:\n",
        "    sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, width * height * c)), y)\n",
        "  X_resampled = X_resampled.reshape(-1, width, height, c)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled\n",
        "\n",
        "def SMOTE_Data2(X, y, one_hot = False, k = 5):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X, y)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qge6cnxQPnH6",
        "outputId": "58a818eb-cdef-4167-a52e-9eecc1675dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5321, 224, 224, 3)\n",
            "(5321, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 1113, 2: 1099, 1: 514, 0: 327, 6: 142, 3: 115})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "df1 = pd.read_pickle(path+\"isic2018_train.pkl\")\n",
        "X_train = df1.loc[:, df1.columns != 'y_train'].to_numpy()\n",
        "X_train = X_train.reshape(-1,224,224,3)\n",
        "y_train = df1.loc[:, df1.columns == 'y_train'].to_numpy()\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "df1 = pd.read_pickle(path+\"isic2018_val.pkl\")\n",
        "X_val = df1.loc[:, df1.columns != 'y_val'].to_numpy()\n",
        "X_val = X_val.reshape(-1,224,224,3)\n",
        "y_val = df1.loc[:, df1.columns == 'y_val'].to_numpy()\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArGWuciBt_-",
        "outputId": "a2a8c7f4-f5a1-4af5-9a64-b272685ea624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14077, 32, 32, 3)\n",
            "(14077, 7)\n",
            "(193, 32, 32, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 2011, 2: 2011, 3: 2011, 0: 2011, 1: 2011, 6: 2011})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V7Z_nccu6QjB"
      },
      "outputs": [],
      "source": [
        "#path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "#df1 = pd.DataFrame(X_train.reshape(X_train.shape[0],-1))\n",
        "#df1['y_train'] = np.argmax(y_train, axis=1).tolist()\n",
        "#df2 = pd.DataFrame(X_val.reshape(X_val.shape[0],-1))\n",
        "#df2['y_val'] = np.argmax(y_val, axis=1).tolist()\n",
        "#df1.to_pickle(path+\"isic2018_train_32.pkl\")\n",
        "#df2.to_pickle(path+\"isic2018_val_32.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nAMBgWqIsAAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae3bd98-457d-41fb-a6d5-4cd90f9c5bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5321, 32, 32, 3)\n",
            "(5321, 7)\n",
            "(193, 32, 32, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 1113, 2: 1099, 1: 514, 0: 327, 6: 142, 3: 115})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vIygrW81Ln4z",
        "outputId": "2b76192e-beaf-4523-d09a-2b3170db14e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.6827 - accuracy: 0.3436 - balanced_acc: 0.3453\n",
            "Epoch 1: val_balanced_acc improved from -inf to 0.18108, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 44s 173ms/step - loss: 1.6827 - accuracy: 0.3436 - balanced_acc: 0.3453 - val_loss: 1.2021 - val_accuracy: 0.6321 - val_balanced_acc: 0.1811 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.4891 - accuracy: 0.4224 - balanced_acc: 0.4234\n",
            "Epoch 2: val_balanced_acc improved from 0.18108 to 0.19264, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 40s 176ms/step - loss: 1.4891 - accuracy: 0.4224 - balanced_acc: 0.4234 - val_loss: 1.0783 - val_accuracy: 0.6425 - val_balanced_acc: 0.1926 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.4125 - accuracy: 0.4484 - balanced_acc: 0.4468\n",
            "Epoch 3: val_balanced_acc did not improve from 0.19264\n",
            "219/219 [==============================] - 37s 170ms/step - loss: 1.4125 - accuracy: 0.4484 - balanced_acc: 0.4468 - val_loss: 1.0328 - val_accuracy: 0.6373 - val_balanced_acc: 0.1897 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.3690 - accuracy: 0.4708 - balanced_acc: 0.4702\n",
            "Epoch 4: val_balanced_acc improved from 0.19264 to 0.22576, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 39s 180ms/step - loss: 1.3690 - accuracy: 0.4708 - balanced_acc: 0.4702 - val_loss: 1.0449 - val_accuracy: 0.6373 - val_balanced_acc: 0.2258 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.3314 - accuracy: 0.4854 - balanced_acc: 0.4838\n",
            "Epoch 5: val_balanced_acc improved from 0.22576 to 0.25021, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 40s 181ms/step - loss: 1.3314 - accuracy: 0.4854 - balanced_acc: 0.4838 - val_loss: 1.0081 - val_accuracy: 0.6528 - val_balanced_acc: 0.2502 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.3046 - accuracy: 0.5007 - balanced_acc: 0.5029\n",
            "Epoch 6: val_balanced_acc did not improve from 0.25021\n",
            "219/219 [==============================] - 38s 175ms/step - loss: 1.3046 - accuracy: 0.5007 - balanced_acc: 0.5029 - val_loss: 1.0003 - val_accuracy: 0.6580 - val_balanced_acc: 0.2334 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.2818 - accuracy: 0.5104 - balanced_acc: 0.5113\n",
            "Epoch 7: val_balanced_acc did not improve from 0.25021\n",
            "219/219 [==============================] - 38s 175ms/step - loss: 1.2818 - accuracy: 0.5104 - balanced_acc: 0.5113 - val_loss: 0.9365 - val_accuracy: 0.6736 - val_balanced_acc: 0.2431 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.2611 - accuracy: 0.5213 - balanced_acc: 0.5201\n",
            "Epoch 8: val_balanced_acc did not improve from 0.25021\n",
            "219/219 [==============================] - 39s 176ms/step - loss: 1.2611 - accuracy: 0.5213 - balanced_acc: 0.5201 - val_loss: 0.9768 - val_accuracy: 0.6580 - val_balanced_acc: 0.2385 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.2409 - accuracy: 0.5289 - balanced_acc: 0.5293\n",
            "Epoch 9: val_balanced_acc improved from 0.25021 to 0.37193, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 1.2409 - accuracy: 0.5289 - balanced_acc: 0.5293 - val_loss: 0.9179 - val_accuracy: 0.6839 - val_balanced_acc: 0.3719 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.2233 - accuracy: 0.5355 - balanced_acc: 0.5350\n",
            "Epoch 10: val_balanced_acc improved from 0.37193 to 0.37713, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 185ms/step - loss: 1.2233 - accuracy: 0.5355 - balanced_acc: 0.5350 - val_loss: 0.9441 - val_accuracy: 0.6528 - val_balanced_acc: 0.3771 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.2145 - accuracy: 0.5421 - balanced_acc: 0.5429\n",
            "Epoch 11: val_balanced_acc did not improve from 0.37713\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 1.2145 - accuracy: 0.5421 - balanced_acc: 0.5429 - val_loss: 0.9145 - val_accuracy: 0.6632 - val_balanced_acc: 0.3677 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1965 - accuracy: 0.5483 - balanced_acc: 0.5466\n",
            "Epoch 12: val_balanced_acc did not improve from 0.37713\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 1.1965 - accuracy: 0.5483 - balanced_acc: 0.5466 - val_loss: 0.9239 - val_accuracy: 0.6528 - val_balanced_acc: 0.3659 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1880 - accuracy: 0.5490 - balanced_acc: 0.5514\n",
            "Epoch 13: val_balanced_acc improved from 0.37713 to 0.38738, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 1.1880 - accuracy: 0.5490 - balanced_acc: 0.5514 - val_loss: 0.8715 - val_accuracy: 0.6788 - val_balanced_acc: 0.3874 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1777 - accuracy: 0.5564 - balanced_acc: 0.5582\n",
            "Epoch 14: val_balanced_acc improved from 0.38738 to 0.39851, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 185ms/step - loss: 1.1777 - accuracy: 0.5564 - balanced_acc: 0.5582 - val_loss: 0.9387 - val_accuracy: 0.6632 - val_balanced_acc: 0.3985 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1582 - accuracy: 0.5624 - balanced_acc: 0.5610\n",
            "Epoch 15: val_balanced_acc did not improve from 0.39851\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 1.1582 - accuracy: 0.5624 - balanced_acc: 0.5610 - val_loss: 0.8996 - val_accuracy: 0.6632 - val_balanced_acc: 0.2462 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1549 - accuracy: 0.5653 - balanced_acc: 0.5656\n",
            "Epoch 16: val_balanced_acc improved from 0.39851 to 0.39910, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 1.1549 - accuracy: 0.5653 - balanced_acc: 0.5656 - val_loss: 0.8918 - val_accuracy: 0.6736 - val_balanced_acc: 0.3991 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1429 - accuracy: 0.5700 - balanced_acc: 0.5729\n",
            "Epoch 17: val_balanced_acc did not improve from 0.39910\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.1429 - accuracy: 0.5700 - balanced_acc: 0.5729 - val_loss: 0.8590 - val_accuracy: 0.6736 - val_balanced_acc: 0.3949 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1337 - accuracy: 0.5734 - balanced_acc: 0.5725\n",
            "Epoch 18: val_balanced_acc did not improve from 0.39910\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.1337 - accuracy: 0.5734 - balanced_acc: 0.5725 - val_loss: 0.8913 - val_accuracy: 0.6528 - val_balanced_acc: 0.2454 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1256 - accuracy: 0.5737 - balanced_acc: 0.5719\n",
            "Epoch 19: val_balanced_acc did not improve from 0.39910\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.1256 - accuracy: 0.5737 - balanced_acc: 0.5719 - val_loss: 0.8560 - val_accuracy: 0.6788 - val_balanced_acc: 0.3815 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1156 - accuracy: 0.5802 - balanced_acc: 0.5804\n",
            "Epoch 20: val_balanced_acc did not improve from 0.39910\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.1156 - accuracy: 0.5802 - balanced_acc: 0.5804 - val_loss: 0.8837 - val_accuracy: 0.6736 - val_balanced_acc: 0.3976 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1063 - accuracy: 0.5845 - balanced_acc: 0.5842\n",
            "Epoch 21: val_balanced_acc did not improve from 0.39910\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.1063 - accuracy: 0.5845 - balanced_acc: 0.5842 - val_loss: 0.8261 - val_accuracy: 0.6891 - val_balanced_acc: 0.3975 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.1043 - accuracy: 0.5844 - balanced_acc: 0.5823\n",
            "Epoch 22: val_balanced_acc improved from 0.39910 to 0.40090, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 1.1043 - accuracy: 0.5844 - balanced_acc: 0.5823 - val_loss: 0.8457 - val_accuracy: 0.6839 - val_balanced_acc: 0.4009 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0882 - accuracy: 0.5902 - balanced_acc: 0.5917\n",
            "Epoch 23: val_balanced_acc did not improve from 0.40090\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 1.0882 - accuracy: 0.5902 - balanced_acc: 0.5917 - val_loss: 0.8570 - val_accuracy: 0.6736 - val_balanced_acc: 0.4003 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0898 - accuracy: 0.5926 - balanced_acc: 0.5931\n",
            "Epoch 24: val_balanced_acc improved from 0.40090 to 0.40277, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 1.0898 - accuracy: 0.5926 - balanced_acc: 0.5931 - val_loss: 0.8523 - val_accuracy: 0.6788 - val_balanced_acc: 0.4028 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0744 - accuracy: 0.5977 - balanced_acc: 0.5979\n",
            "Epoch 25: val_balanced_acc did not improve from 0.40277\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0744 - accuracy: 0.5977 - balanced_acc: 0.5979 - val_loss: 0.8520 - val_accuracy: 0.6684 - val_balanced_acc: 0.3977 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0767 - accuracy: 0.5961 - balanced_acc: 0.5943\n",
            "Epoch 26: val_balanced_acc did not improve from 0.40277\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0767 - accuracy: 0.5961 - balanced_acc: 0.5943 - val_loss: 0.8407 - val_accuracy: 0.6788 - val_balanced_acc: 0.3867 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0616 - accuracy: 0.6026 - balanced_acc: 0.5999\n",
            "Epoch 27: val_balanced_acc improved from 0.40277 to 0.40536, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 189ms/step - loss: 1.0616 - accuracy: 0.6026 - balanced_acc: 0.5999 - val_loss: 0.8202 - val_accuracy: 0.6891 - val_balanced_acc: 0.4054 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0562 - accuracy: 0.6067 - balanced_acc: 0.6084\n",
            "Epoch 28: val_balanced_acc did not improve from 0.40536\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 1.0562 - accuracy: 0.6067 - balanced_acc: 0.6084 - val_loss: 0.8416 - val_accuracy: 0.6528 - val_balanced_acc: 0.2477 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0484 - accuracy: 0.6101 - balanced_acc: 0.6066\n",
            "Epoch 29: val_balanced_acc did not improve from 0.40536\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 1.0484 - accuracy: 0.6101 - balanced_acc: 0.6066 - val_loss: 0.8058 - val_accuracy: 0.6788 - val_balanced_acc: 0.3830 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0514 - accuracy: 0.6034 - balanced_acc: 0.6068\n",
            "Epoch 30: val_balanced_acc did not improve from 0.40536\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0514 - accuracy: 0.6034 - balanced_acc: 0.6068 - val_loss: 0.8254 - val_accuracy: 0.6839 - val_balanced_acc: 0.4039 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0387 - accuracy: 0.6106 - balanced_acc: 0.6098\n",
            "Epoch 31: val_balanced_acc improved from 0.40536 to 0.42254, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 1.0387 - accuracy: 0.6106 - balanced_acc: 0.6098 - val_loss: 0.8342 - val_accuracy: 0.6736 - val_balanced_acc: 0.4225 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0292 - accuracy: 0.6160 - balanced_acc: 0.6179\n",
            "Epoch 32: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0292 - accuracy: 0.6160 - balanced_acc: 0.6179 - val_loss: 0.7943 - val_accuracy: 0.6943 - val_balanced_acc: 0.4069 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0325 - accuracy: 0.6125 - balanced_acc: 0.6136\n",
            "Epoch 33: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0325 - accuracy: 0.6125 - balanced_acc: 0.6136 - val_loss: 0.8124 - val_accuracy: 0.6839 - val_balanced_acc: 0.2649 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0261 - accuracy: 0.6172 - balanced_acc: 0.6161\n",
            "Epoch 34: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0261 - accuracy: 0.6172 - balanced_acc: 0.6161 - val_loss: 0.8330 - val_accuracy: 0.6788 - val_balanced_acc: 0.3901 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0152 - accuracy: 0.6231 - balanced_acc: 0.6234\n",
            "Epoch 35: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0152 - accuracy: 0.6231 - balanced_acc: 0.6234 - val_loss: 0.8187 - val_accuracy: 0.6788 - val_balanced_acc: 0.2683 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0105 - accuracy: 0.6264 - balanced_acc: 0.6300\n",
            "Epoch 36: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0105 - accuracy: 0.6264 - balanced_acc: 0.6300 - val_loss: 0.7893 - val_accuracy: 0.6995 - val_balanced_acc: 0.4063 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0087 - accuracy: 0.6228 - balanced_acc: 0.6243\n",
            "Epoch 37: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0087 - accuracy: 0.6228 - balanced_acc: 0.6243 - val_loss: 0.8147 - val_accuracy: 0.6788 - val_balanced_acc: 0.4085 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9928 - accuracy: 0.6341 - balanced_acc: 0.6352\n",
            "Epoch 38: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9928 - accuracy: 0.6341 - balanced_acc: 0.6352 - val_loss: 0.8194 - val_accuracy: 0.6684 - val_balanced_acc: 0.4020 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 1.0011 - accuracy: 0.6276 - balanced_acc: 0.6246\n",
            "Epoch 39: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 1.0011 - accuracy: 0.6276 - balanced_acc: 0.6246 - val_loss: 0.7903 - val_accuracy: 0.7047 - val_balanced_acc: 0.4129 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9900 - accuracy: 0.6291 - balanced_acc: 0.6311\n",
            "Epoch 40: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9900 - accuracy: 0.6291 - balanced_acc: 0.6311 - val_loss: 0.8269 - val_accuracy: 0.6632 - val_balanced_acc: 0.3990 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9844 - accuracy: 0.6334 - balanced_acc: 0.6335\n",
            "Epoch 41: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9844 - accuracy: 0.6334 - balanced_acc: 0.6335 - val_loss: 0.7818 - val_accuracy: 0.6943 - val_balanced_acc: 0.4124 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9837 - accuracy: 0.6301 - balanced_acc: 0.6296\n",
            "Epoch 42: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9837 - accuracy: 0.6301 - balanced_acc: 0.6296 - val_loss: 0.7997 - val_accuracy: 0.6839 - val_balanced_acc: 0.4061 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9759 - accuracy: 0.6380 - balanced_acc: 0.6362\n",
            "Epoch 43: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 0.9759 - accuracy: 0.6380 - balanced_acc: 0.6362 - val_loss: 0.7948 - val_accuracy: 0.6943 - val_balanced_acc: 0.4040 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9753 - accuracy: 0.6403 - balanced_acc: 0.6394\n",
            "Epoch 44: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9753 - accuracy: 0.6403 - balanced_acc: 0.6394 - val_loss: 0.7696 - val_accuracy: 0.7047 - val_balanced_acc: 0.4093 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9634 - accuracy: 0.6415 - balanced_acc: 0.6415\n",
            "Epoch 45: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9634 - accuracy: 0.6415 - balanced_acc: 0.6415 - val_loss: 0.8009 - val_accuracy: 0.7047 - val_balanced_acc: 0.4156 - lr: 0.0010\n",
            "Epoch 46/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9612 - accuracy: 0.6442 - balanced_acc: 0.6460\n",
            "Epoch 46: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9612 - accuracy: 0.6442 - balanced_acc: 0.6460 - val_loss: 0.8155 - val_accuracy: 0.6684 - val_balanced_acc: 0.3856 - lr: 0.0010\n",
            "Epoch 47/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9648 - accuracy: 0.6398 - balanced_acc: 0.6405\n",
            "Epoch 47: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9648 - accuracy: 0.6398 - balanced_acc: 0.6405 - val_loss: 0.7703 - val_accuracy: 0.7098 - val_balanced_acc: 0.4171 - lr: 0.0010\n",
            "Epoch 48/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9508 - accuracy: 0.6462 - balanced_acc: 0.6470\n",
            "Epoch 48: val_balanced_acc did not improve from 0.42254\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9508 - accuracy: 0.6462 - balanced_acc: 0.6470 - val_loss: 0.7815 - val_accuracy: 0.6943 - val_balanced_acc: 0.4121 - lr: 0.0010\n",
            "Epoch 49/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9550 - accuracy: 0.6443 - balanced_acc: 0.6425\n",
            "Epoch 49: val_balanced_acc improved from 0.42254 to 0.42488, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.9550 - accuracy: 0.6443 - balanced_acc: 0.6425 - val_loss: 0.8008 - val_accuracy: 0.6943 - val_balanced_acc: 0.4249 - lr: 0.0010\n",
            "Epoch 50/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9449 - accuracy: 0.6463 - balanced_acc: 0.6443\n",
            "Epoch 50: val_balanced_acc did not improve from 0.42488\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9449 - accuracy: 0.6463 - balanced_acc: 0.6443 - val_loss: 0.7311 - val_accuracy: 0.7202 - val_balanced_acc: 0.4238 - lr: 0.0010\n",
            "Epoch 51/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9344 - accuracy: 0.6545 - balanced_acc: 0.6544\n",
            "Epoch 51: val_balanced_acc did not improve from 0.42488\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9344 - accuracy: 0.6545 - balanced_acc: 0.6544 - val_loss: 0.7473 - val_accuracy: 0.6943 - val_balanced_acc: 0.3991 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9367 - accuracy: 0.6553 - balanced_acc: 0.6539\n",
            "Epoch 52: val_balanced_acc did not improve from 0.42488\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9367 - accuracy: 0.6553 - balanced_acc: 0.6539 - val_loss: 0.7640 - val_accuracy: 0.7098 - val_balanced_acc: 0.4194 - lr: 0.0010\n",
            "Epoch 53/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9375 - accuracy: 0.6515 - balanced_acc: 0.6517\n",
            "Epoch 53: val_balanced_acc did not improve from 0.42488\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 0.9375 - accuracy: 0.6515 - balanced_acc: 0.6517 - val_loss: 0.7660 - val_accuracy: 0.6943 - val_balanced_acc: 0.4042 - lr: 0.0010\n",
            "Epoch 54/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9221 - accuracy: 0.6584 - balanced_acc: 0.6582\n",
            "Epoch 54: val_balanced_acc improved from 0.42488 to 0.44337, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.9221 - accuracy: 0.6584 - balanced_acc: 0.6582 - val_loss: 0.7711 - val_accuracy: 0.7098 - val_balanced_acc: 0.4434 - lr: 0.0010\n",
            "Epoch 55/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9312 - accuracy: 0.6540 - balanced_acc: 0.6527\n",
            "Epoch 55: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 0.9312 - accuracy: 0.6540 - balanced_acc: 0.6527 - val_loss: 0.7928 - val_accuracy: 0.6995 - val_balanced_acc: 0.4220 - lr: 0.0010\n",
            "Epoch 56/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.6642 - balanced_acc: 0.6650\n",
            "Epoch 56: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 0.9124 - accuracy: 0.6642 - balanced_acc: 0.6650 - val_loss: 0.7535 - val_accuracy: 0.7254 - val_balanced_acc: 0.4388 - lr: 0.0010\n",
            "Epoch 57/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9212 - accuracy: 0.6595 - balanced_acc: 0.6586\n",
            "Epoch 57: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9212 - accuracy: 0.6595 - balanced_acc: 0.6586 - val_loss: 0.7884 - val_accuracy: 0.6891 - val_balanced_acc: 0.4160 - lr: 0.0010\n",
            "Epoch 58/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9096 - accuracy: 0.6640 - balanced_acc: 0.6620\n",
            "Epoch 58: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9096 - accuracy: 0.6640 - balanced_acc: 0.6620 - val_loss: 0.8034 - val_accuracy: 0.6943 - val_balanced_acc: 0.4142 - lr: 0.0010\n",
            "Epoch 59/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9144 - accuracy: 0.6591 - balanced_acc: 0.6560\n",
            "Epoch 59: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9144 - accuracy: 0.6591 - balanced_acc: 0.6560 - val_loss: 0.7862 - val_accuracy: 0.7047 - val_balanced_acc: 0.4336 - lr: 0.0010\n",
            "Epoch 60/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9009 - accuracy: 0.6671 - balanced_acc: 0.6700\n",
            "Epoch 60: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9009 - accuracy: 0.6671 - balanced_acc: 0.6700 - val_loss: 0.7506 - val_accuracy: 0.7098 - val_balanced_acc: 0.4314 - lr: 0.0010\n",
            "Epoch 61/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.9009 - accuracy: 0.6695 - balanced_acc: 0.6692\n",
            "Epoch 61: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.9009 - accuracy: 0.6695 - balanced_acc: 0.6692 - val_loss: 0.8239 - val_accuracy: 0.6839 - val_balanced_acc: 0.4123 - lr: 0.0010\n",
            "Epoch 62/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8966 - accuracy: 0.6697 - balanced_acc: 0.6694\n",
            "Epoch 62: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8966 - accuracy: 0.6697 - balanced_acc: 0.6694 - val_loss: 0.7783 - val_accuracy: 0.7098 - val_balanced_acc: 0.4398 - lr: 0.0010\n",
            "Epoch 63/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8922 - accuracy: 0.6721 - balanced_acc: 0.6723\n",
            "Epoch 63: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8922 - accuracy: 0.6721 - balanced_acc: 0.6723 - val_loss: 0.7751 - val_accuracy: 0.6995 - val_balanced_acc: 0.4139 - lr: 0.0010\n",
            "Epoch 64/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8863 - accuracy: 0.6725 - balanced_acc: 0.6722\n",
            "Epoch 64: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8863 - accuracy: 0.6725 - balanced_acc: 0.6722 - val_loss: 0.7573 - val_accuracy: 0.6943 - val_balanced_acc: 0.4040 - lr: 0.0010\n",
            "Epoch 65/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8909 - accuracy: 0.6674 - balanced_acc: 0.6664\n",
            "Epoch 65: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8909 - accuracy: 0.6674 - balanced_acc: 0.6664 - val_loss: 0.7583 - val_accuracy: 0.6995 - val_balanced_acc: 0.4270 - lr: 0.0010\n",
            "Epoch 66/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8769 - accuracy: 0.6764 - balanced_acc: 0.6759\n",
            "Epoch 66: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8769 - accuracy: 0.6764 - balanced_acc: 0.6759 - val_loss: 0.7613 - val_accuracy: 0.7098 - val_balanced_acc: 0.4307 - lr: 0.0010\n",
            "Epoch 67/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8860 - accuracy: 0.6757 - balanced_acc: 0.6766\n",
            "Epoch 67: val_balanced_acc did not improve from 0.44337\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8860 - accuracy: 0.6757 - balanced_acc: 0.6766 - val_loss: 0.7555 - val_accuracy: 0.7098 - val_balanced_acc: 0.4279 - lr: 0.0010\n",
            "Epoch 68/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8726 - accuracy: 0.6742 - balanced_acc: 0.6752\n",
            "Epoch 68: val_balanced_acc improved from 0.44337 to 0.45054, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 186ms/step - loss: 0.8726 - accuracy: 0.6742 - balanced_acc: 0.6752 - val_loss: 0.7823 - val_accuracy: 0.6995 - val_balanced_acc: 0.4505 - lr: 0.0010\n",
            "Epoch 69/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8765 - accuracy: 0.6777 - balanced_acc: 0.6775\n",
            "Epoch 69: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8765 - accuracy: 0.6777 - balanced_acc: 0.6775 - val_loss: 0.7478 - val_accuracy: 0.7047 - val_balanced_acc: 0.4163 - lr: 0.0010\n",
            "Epoch 70/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8590 - accuracy: 0.6817 - balanced_acc: 0.6820\n",
            "Epoch 70: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8590 - accuracy: 0.6817 - balanced_acc: 0.6820 - val_loss: 0.7548 - val_accuracy: 0.7098 - val_balanced_acc: 0.4209 - lr: 0.0010\n",
            "Epoch 71/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8652 - accuracy: 0.6827 - balanced_acc: 0.6795\n",
            "Epoch 71: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8652 - accuracy: 0.6827 - balanced_acc: 0.6795 - val_loss: 0.7671 - val_accuracy: 0.7098 - val_balanced_acc: 0.4270 - lr: 0.0010\n",
            "Epoch 72/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8686 - accuracy: 0.6823 - balanced_acc: 0.6836\n",
            "Epoch 72: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8686 - accuracy: 0.6823 - balanced_acc: 0.6836 - val_loss: 0.7430 - val_accuracy: 0.7047 - val_balanced_acc: 0.4079 - lr: 0.0010\n",
            "Epoch 73/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8497 - accuracy: 0.6894 - balanced_acc: 0.6882\n",
            "Epoch 73: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8497 - accuracy: 0.6894 - balanced_acc: 0.6882 - val_loss: 0.7595 - val_accuracy: 0.7098 - val_balanced_acc: 0.4344 - lr: 0.0010\n",
            "Epoch 74/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8582 - accuracy: 0.6873 - balanced_acc: 0.6892\n",
            "Epoch 74: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8582 - accuracy: 0.6873 - balanced_acc: 0.6892 - val_loss: 0.7283 - val_accuracy: 0.7202 - val_balanced_acc: 0.4075 - lr: 0.0010\n",
            "Epoch 75/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8500 - accuracy: 0.6878 - balanced_acc: 0.6871\n",
            "Epoch 75: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8500 - accuracy: 0.6878 - balanced_acc: 0.6871 - val_loss: 0.7593 - val_accuracy: 0.7150 - val_balanced_acc: 0.4442 - lr: 0.0010\n",
            "Epoch 76/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8432 - accuracy: 0.6874 - balanced_acc: 0.6887\n",
            "Epoch 76: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8432 - accuracy: 0.6874 - balanced_acc: 0.6887 - val_loss: 0.7787 - val_accuracy: 0.6995 - val_balanced_acc: 0.4166 - lr: 0.0010\n",
            "Epoch 77/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8490 - accuracy: 0.6873 - balanced_acc: 0.6902\n",
            "Epoch 77: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 178ms/step - loss: 0.8490 - accuracy: 0.6873 - balanced_acc: 0.6902 - val_loss: 0.7656 - val_accuracy: 0.7047 - val_balanced_acc: 0.4324 - lr: 0.0010\n",
            "Epoch 78/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8408 - accuracy: 0.6966 - balanced_acc: 0.6972\n",
            "Epoch 78: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8408 - accuracy: 0.6966 - balanced_acc: 0.6972 - val_loss: 0.7733 - val_accuracy: 0.7047 - val_balanced_acc: 0.4499 - lr: 0.0010\n",
            "Epoch 79/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8330 - accuracy: 0.6924 - balanced_acc: 0.6890\n",
            "Epoch 79: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8330 - accuracy: 0.6924 - balanced_acc: 0.6890 - val_loss: 0.7377 - val_accuracy: 0.6995 - val_balanced_acc: 0.4139 - lr: 0.0010\n",
            "Epoch 80/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8319 - accuracy: 0.6999 - balanced_acc: 0.7002\n",
            "Epoch 80: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8319 - accuracy: 0.6999 - balanced_acc: 0.7002 - val_loss: 0.7660 - val_accuracy: 0.7047 - val_balanced_acc: 0.4175 - lr: 0.0010\n",
            "Epoch 81/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8346 - accuracy: 0.6934 - balanced_acc: 0.6926\n",
            "Epoch 81: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8346 - accuracy: 0.6934 - balanced_acc: 0.6926 - val_loss: 0.7500 - val_accuracy: 0.7150 - val_balanced_acc: 0.4279 - lr: 0.0010\n",
            "Epoch 82/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8260 - accuracy: 0.7001 - balanced_acc: 0.6990\n",
            "Epoch 82: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8260 - accuracy: 0.7001 - balanced_acc: 0.6990 - val_loss: 0.7593 - val_accuracy: 0.7047 - val_balanced_acc: 0.4271 - lr: 0.0010\n",
            "Epoch 83/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8267 - accuracy: 0.6942 - balanced_acc: 0.6939\n",
            "Epoch 83: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8267 - accuracy: 0.6942 - balanced_acc: 0.6939 - val_loss: 0.7692 - val_accuracy: 0.6995 - val_balanced_acc: 0.4220 - lr: 0.0010\n",
            "Epoch 84/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8150 - accuracy: 0.7036 - balanced_acc: 0.7048\n",
            "Epoch 84: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8150 - accuracy: 0.7036 - balanced_acc: 0.7048 - val_loss: 0.7557 - val_accuracy: 0.7098 - val_balanced_acc: 0.4279 - lr: 0.0010\n",
            "Epoch 85/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8235 - accuracy: 0.6991 - balanced_acc: 0.6973\n",
            "Epoch 85: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8235 - accuracy: 0.6991 - balanced_acc: 0.6973 - val_loss: 0.7679 - val_accuracy: 0.7150 - val_balanced_acc: 0.4315 - lr: 0.0010\n",
            "Epoch 86/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8164 - accuracy: 0.7005 - balanced_acc: 0.6994\n",
            "Epoch 86: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8164 - accuracy: 0.7005 - balanced_acc: 0.6994 - val_loss: 0.7404 - val_accuracy: 0.7150 - val_balanced_acc: 0.4174 - lr: 0.0010\n",
            "Epoch 87/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8057 - accuracy: 0.7062 - balanced_acc: 0.7076\n",
            "Epoch 87: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8057 - accuracy: 0.7062 - balanced_acc: 0.7076 - val_loss: 0.7174 - val_accuracy: 0.7513 - val_balanced_acc: 0.4234 - lr: 0.0010\n",
            "Epoch 88/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8112 - accuracy: 0.7044 - balanced_acc: 0.7072\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 88: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8112 - accuracy: 0.7044 - balanced_acc: 0.7072 - val_loss: 0.7280 - val_accuracy: 0.7150 - val_balanced_acc: 0.4270 - lr: 0.0010\n",
            "Epoch 89/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.8018 - accuracy: 0.7100 - balanced_acc: 0.7100\n",
            "Epoch 89: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.8018 - accuracy: 0.7100 - balanced_acc: 0.7100 - val_loss: 0.7417 - val_accuracy: 0.7047 - val_balanced_acc: 0.4160 - lr: 5.0000e-04\n",
            "Epoch 90/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7947 - accuracy: 0.7129 - balanced_acc: 0.7105\n",
            "Epoch 90: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7947 - accuracy: 0.7129 - balanced_acc: 0.7105 - val_loss: 0.7580 - val_accuracy: 0.7150 - val_balanced_acc: 0.4485 - lr: 5.0000e-04\n",
            "Epoch 91/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7954 - accuracy: 0.7123 - balanced_acc: 0.7151\n",
            "Epoch 91: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7954 - accuracy: 0.7123 - balanced_acc: 0.7151 - val_loss: 0.7250 - val_accuracy: 0.7047 - val_balanced_acc: 0.4228 - lr: 5.0000e-04\n",
            "Epoch 92/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7879 - accuracy: 0.7154 - balanced_acc: 0.7133\n",
            "Epoch 92: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7879 - accuracy: 0.7154 - balanced_acc: 0.7133 - val_loss: 0.7747 - val_accuracy: 0.7202 - val_balanced_acc: 0.4351 - lr: 5.0000e-04\n",
            "Epoch 93/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7922 - accuracy: 0.7178 - balanced_acc: 0.7193\n",
            "Epoch 93: val_balanced_acc did not improve from 0.45054\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7922 - accuracy: 0.7178 - balanced_acc: 0.7193 - val_loss: 0.7651 - val_accuracy: 0.7150 - val_balanced_acc: 0.4270 - lr: 5.0000e-04\n",
            "Epoch 94/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7944 - accuracy: 0.7078 - balanced_acc: 0.7044\n",
            "Epoch 94: val_balanced_acc improved from 0.45054 to 0.45403, saving model to /content/drive/MyDrive/PHD/Model/best_model_attention.h5\n",
            "219/219 [==============================] - 41s 187ms/step - loss: 0.7944 - accuracy: 0.7078 - balanced_acc: 0.7044 - val_loss: 0.7422 - val_accuracy: 0.7150 - val_balanced_acc: 0.4540 - lr: 5.0000e-04\n",
            "Epoch 95/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7862 - accuracy: 0.7175 - balanced_acc: 0.7190\n",
            "Epoch 95: val_balanced_acc did not improve from 0.45403\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7862 - accuracy: 0.7175 - balanced_acc: 0.7190 - val_loss: 0.7506 - val_accuracy: 0.7047 - val_balanced_acc: 0.4175 - lr: 5.0000e-04\n",
            "Epoch 96/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7843 - accuracy: 0.7180 - balanced_acc: 0.7189\n",
            "Epoch 96: val_balanced_acc did not improve from 0.45403\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7843 - accuracy: 0.7180 - balanced_acc: 0.7189 - val_loss: 0.7346 - val_accuracy: 0.7047 - val_balanced_acc: 0.4190 - lr: 5.0000e-04\n",
            "Epoch 97/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7867 - accuracy: 0.7168 - balanced_acc: 0.7146\n",
            "Epoch 97: val_balanced_acc did not improve from 0.45403\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7867 - accuracy: 0.7168 - balanced_acc: 0.7146 - val_loss: 0.7214 - val_accuracy: 0.7306 - val_balanced_acc: 0.4276 - lr: 5.0000e-04\n",
            "Epoch 98/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7833 - accuracy: 0.7168 - balanced_acc: 0.7183\n",
            "Epoch 98: val_balanced_acc did not improve from 0.45403\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7833 - accuracy: 0.7168 - balanced_acc: 0.7183 - val_loss: 0.7689 - val_accuracy: 0.7098 - val_balanced_acc: 0.4291 - lr: 5.0000e-04\n",
            "Epoch 99/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7828 - accuracy: 0.7145 - balanced_acc: 0.7146\n",
            "Epoch 99: val_balanced_acc did not improve from 0.45403\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7828 - accuracy: 0.7145 - balanced_acc: 0.7146 - val_loss: 0.7680 - val_accuracy: 0.7047 - val_balanced_acc: 0.4184 - lr: 5.0000e-04\n",
            "Epoch 100/100\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.7836 - accuracy: 0.7179 - balanced_acc: 0.7173\n",
            "Epoch 100: val_balanced_acc did not improve from 0.45403\n",
            "219/219 [==============================] - 39s 179ms/step - loss: 0.7836 - accuracy: 0.7179 - balanced_acc: 0.7173 - val_loss: 0.7575 - val_accuracy: 0.7150 - val_balanced_acc: 0.4270 - lr: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# define model\n",
        "model = define_base_model('resnet50')\n",
        "#model.summary()\n",
        "hst = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n",
        "# learning curves\n",
        "summarize_diagnostics(hst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vXnW3lmCgln3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ef6ea12f-dc35-4179-d5a5-e6f8d499c628"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwwLiXUSG0IZ"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "#hst = model.fit(train_data_batches,\n",
        "#                    epochs = EPOCHS, validation_data = valid_data_batches,      \n",
        "                    #steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "#                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgjmi-4UIT-"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SPz8NH1Oylv9"
      },
      "outputs": [],
      "source": [
        "#save last model\n",
        "model.save(last_model_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS3ewyxO_anU",
        "outputId": "a382a043-1890-4ea4-d374-0ce593354e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.7212474248774596\n",
            "balanced accuracy on training 0.7212474248774596\n",
            "accuracy on validation 0.7150259067357513\n",
            "balanced accuracy on validation 0.4743189737092176\n",
            "Score on val data:  (0.4326844723903548, 0.4743189737092176, 0.44572413687233936, None)\n"
          ]
        }
      ],
      "source": [
        "last_model = load_model(last_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = last_model.predict(X_train)\n",
        "y_val_pred = last_model.predict(X_val)\n",
        "\n",
        "#print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3IyWjdGG4Xq",
        "outputId": "f32586a1-947d-4bdd-f504-255a242d4115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 0.7178376074447681\n",
            "balanced accuracy on training 0.7178376074447682\n",
            "accuracy on validation 0.7150259067357513\n",
            "balanced accuracy on validation 0.4779386699247326\n",
            "Score on val data:  (0.4257849293563579, 0.4779386699247326, 0.4421298814042559, None)\n"
          ]
        }
      ],
      "source": [
        "best_model = load_model(best_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRWiTnO0MGh"
      },
      "source": [
        "#Cut-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGnCoIdLyDHS"
      },
      "outputs": [],
      "source": [
        "df_val_pred = pd.DataFrame(y_val_pred, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdyCbloQyWTC"
      },
      "outputs": [],
      "source": [
        "numbers = [float(x)/40 for x in range(11)]\n",
        "for i in numbers:\n",
        "    df_val_pred[i]= df_val_pred.MEL.map(lambda x: 1 if x > i else 0)\n",
        "df_val_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4SQsRx73kgk"
      },
      "outputs": [],
      "source": [
        "y_val_true= [1 if x == 4 else 0 for x in np.argmax(y_val, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcUISWFi0J05"
      },
      "outputs": [],
      "source": [
        "#num = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
        "cutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n",
        "for i in numbers:\n",
        "    cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "    total1=sum(sum(cm1))\n",
        "    Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31LSzov1tCt"
      },
      "outputs": [],
      "source": [
        "cutoff_df[['Accuracy','Sensitivity','Specificity']].plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CIKT94Jqye"
      },
      "outputs": [],
      "source": [
        "i = 0.025\n",
        "cm1 = confusion_matrix(y_val_true, df_val_pred[i])\n",
        "total1=sum(sum(cm1))\n",
        "Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U2tkFebL_VC"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ', Accuracy)\n",
        "print('Sensitivity: ', Sensitivity)\n",
        "print('Specificity: ', Specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkPOFLehOmFg"
      },
      "outputs": [],
      "source": [
        "#change melanoma flag back to 4\n",
        "df_val_pred[df_val_pred[i] == 1] = 4\n",
        "#decode one-hot y_val_pred while use cut-off melanoma data\n",
        "condition = df_val_pred[i] == 4\n",
        "y_val_pred2 = np.where(condition, df_val_pred[i], np.argmax(y_val_pred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOVl6dWlTDLo"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ',accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))\n",
        "print('Balanced accuracy: ',balanced_accuracy_score(np.argmax(y_val, axis=1), y_val_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvYutTKRhR_"
      },
      "outputs": [],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_val, axis=1), y_val_pred2)\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtvW3YeaLlC"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-1yjWGeKs7"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "#labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "#f = plt.figure(figsize=(15, 6))\n",
        "#s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "#s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic95mefkpG3"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "df_test['image_px'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60LYAT7VsNOZ"
      },
      "outputs": [],
      "source": [
        "X_test = np.asarray(df_test['image_px'].tolist())\n",
        "print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXnnIIwC4cHE"
      },
      "outputs": [],
      "source": [
        "#preprocess\n",
        "X_test = preprocess_image_input(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeDTXdaMLmyU"
      },
      "outputs": [],
      "source": [
        "#X_test = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIX0AmEFNv3Y"
      },
      "outputs": [],
      "source": [
        "# predicting\n",
        "#CHANGE THE MODEL IF NECESSARY\n",
        "#X_test2 = model1.predict(X_test)\n",
        "Y_pred2 = model.predict(X_test2)\n",
        "print(\"Y_pred2\", Y_pred2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]\n",
        "df_pred.set_index(\"image\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ynyd8PjT589"
      },
      "outputs": [],
      "source": [
        "#update MEL data using cut-off value\n",
        "df_pred.MEL[df_pred.MEL > i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjRdONoQVMq0"
      },
      "outputs": [],
      "source": [
        "df_pred.loc[df_pred.MEL > i, ['NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_SMOTE_Attention.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0MghVs0tsGw"
      },
      "source": [
        "result: 0.656"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswA0co2y1wl"
      },
      "source": [
        "#Exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnqJYIONy34l"
      },
      "outputs": [],
      "source": [
        "input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "base_model = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "x = base_model(input_tensor, training=False)\n",
        "x = Attention(2048,2048,7,8)(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "res50 = Model(inputs=input_tensor, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcn8hQg3J8yP"
      },
      "outputs": [],
      "source": [
        "#Train i-last layer\n",
        "# summarize feature map shapes\n",
        "for i in range(len(model.layers)):\n",
        "    layer = model.layers[i]\n",
        "    # summarize output shape\n",
        "    print(i, layer.name, layer.output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA7Af2Y73FUv"
      },
      "outputs": [],
      "source": [
        "X_train = res50.predict(X_train)\n",
        "X_val = res50.predict(X_val)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krJiAb1m3QNf"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = SMOTE_Data2(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfcFpsBwM0d4"
      },
      "source": [
        "#Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_s6OIGKM26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "371bd24a-4bf9-491a-e0ec-78b7eb06e6d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ff488085aaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_current_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_preprocessing_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Image preprocessing layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m \u001b[0mcython_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'core'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tk\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,AveragePooling2D,BatchNormalization,Add,ZeroPadding2D,Flatten,Dense,Input,LeakyReLU,Softmax,ReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class Attention(tk.layers.Layer):\n",
        "    \n",
        "    def __init__(self,input_channels,output_channel,kernel_size,groups):\n",
        "        super().__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channel = output_channel    \n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = 1\n",
        "        self.groups = groups\n",
        "\n",
        "        assert output_channel % groups == 0\n",
        "        \n",
        "        self.rel_h = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,kernel_size,1,output_channel//2),stddev = 0.1)) \n",
        "        #output_channels//2 is the number of channels on which the relative position will be considered,1 denotes the number of those filters and the one after that too and (kernel_size,1) denotes the size of that filter\n",
        "        self.rel_w = tk.backend.variable(lambda : tk.backend.truncated_normal((1,1,1,kernel_size,output_channel//2),stddev = 0.1)) \n",
        "\n",
        "        self.key_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.query_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "        self.value_weights = Conv2D(self.output_channel,kernel_size = 1,strides = self.stride)\n",
        "\n",
        "    def call(self,x):\n",
        "        \n",
        "        batch,height,width,channels = x.shape\n",
        "        x_padded = ZeroPadding2D(padding=(self.kernel_size//2,self.kernel_size//2))(x)\n",
        "        query = self.query_weights(x)\n",
        "        value = self.value_weights(x_padded)\n",
        "        key = self.key_weights(x_padded)\n",
        "        #key,query and value will have the shape of (batch,height,width,depth)\n",
        "        keys = tf.image.extract_patches(images = key,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        value = tf.image.extract_patches(images = value,sizes = [1,self.kernel_size,self.kernel_size,1],strides = [1,self.stride,self.stride,1],rates = [1,1,1,1], padding = \"VALID\")\n",
        "        no_of_kernels = key.shape[-2] - self.kernel_size + 1\n",
        "        keys = tf.reshape(keys,shape = (-1,no_of_kernels, no_of_kernels,self.kernel_size,self.kernel_size,self.output_channel))\n",
        "        key_split_h,key_split_w = tf.split(keys,num_or_size_splits = 2,axis = -1)\n",
        "        key_with_rel = tk.layers.concatenate([key_split_h + self.rel_h,key_split_w + self.rel_w],axis = -1) \n",
        "        \n",
        "        #reshaping the query and key\n",
        "        key_with_rel = tf.reshape(key_with_rel,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        query  = tf.reshape(query,(-1,self.groups,no_of_kernels,no_of_kernels,1,self.output_channel//self.groups))        \n",
        "        value = tf.reshape(value,(-1,self.groups,no_of_kernels,no_of_kernels,self.kernel_size*self.kernel_size,self.output_channel//self.groups))\n",
        "        \n",
        "        #multiplication  of key and query\n",
        "        #assert key_with_rel.shape == query.shape        \n",
        "        key_prod_query = query*key_with_rel\n",
        "        \n",
        "        # Now the function is passed through the softmax and is multiplied with the values\n",
        "        s = Softmax(axis = -2)(key_prod_query)\n",
        "        y = tf.einsum('bnchwk,bnchwk->bnchk',s,value)\n",
        "        y = tf.reshape(y,(-1,height,width,self.output_channel))\n",
        "        return y\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            \"input_channels\": self.input_channels, \n",
        "            \"output_channel\": self.output_channel, \n",
        "            \"kernel_size\": self.kernel_size, \n",
        "            \"stride\": self.stride, \n",
        "            \"groups\": self.groups, \n",
        "            \"rel_h\": self.rel_h, \n",
        "            \"rel_w\": self.rel_w, \n",
        "            \"key_weights\": self.key_weights, \n",
        "            \"query_weights\": self.query_weights, \n",
        "            \"value_weights\": self.value_weights\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE8Ziq-BlEP4"
      },
      "source": [
        "#Oversampling on feature map level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtgmvyhCndpB"
      },
      "outputs": [],
      "source": [
        "i = 176"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm05Zet_B5am"
      },
      "outputs": [],
      "source": [
        "for i in range(len(model.layers)):\n",
        "  layer = model.layers[i]\n",
        "  print(i, layer.name, layer.output_shape, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqeSic6NmLsR"
      },
      "outputs": [],
      "source": [
        "# redefine model to output right after the first hidden layer\n",
        "model1 = Model(inputs=model.inputs, outputs=model.layers[i].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVHYG9Rwm28i"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "X_train_fm = model1.predict(X_train)\n",
        "X_val_fm = model1.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNozN8-wDUNL"
      },
      "outputs": [],
      "source": [
        "X_train_fm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19hK7aQNeAQo"
      },
      "outputs": [],
      "source": [
        "X_train_fm_ov, y_train_ov = SMOTE_Data2(X_train_fm, y_train, True, 5)\n",
        "print(X_train_fm_ov.shape)\n",
        "print(y_train_ov.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train_ov, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qP4iyYcnAYa"
      },
      "outputs": [],
      "source": [
        "model2 = Model(inputs=model.layers[i].output, outputs=model.layers[len(model.layers)-1].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzdjs0WbvDB0"
      },
      "outputs": [],
      "source": [
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/best_model_no.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/Feature-Map-Ov/last_model_no.h5'\n",
        "model2.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst = model2.fit(X_train_fm_ov, y_train_ov, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val_fm, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train_fm_ov.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XhlbWn--8Or"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW-_U6vFpIci"
      },
      "outputs": [],
      "source": [
        "# get feature map for first hidden layer\n",
        "y_train_pred = model2.predict(X_train_fm_ov)\n",
        "y_val_pred = model2.predict(X_val_fm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLop0YK-ZK40"
      },
      "outputs": [],
      "source": [
        "print('accuracy on training',accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train_ov, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load ISIC 2018 Challange Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l3P7IjyLuZGY"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_isic2018_dataset(train_under_frac = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IncA-_o_n5w"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = y_train.value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(y_train == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "labels = sorted(df1['y_train'].unique())\n",
        "for label in range(7):\n",
        "    plot_images_per_label(df1, label, 3, (12,9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asV1O58Lrq-R"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.fromarray(X_train[0], 'RGB')\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "#df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "#df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "#df_train = df_train.drop_duplicates(subset=['lesion_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmX_Uqbmj-tN"
      },
      "outputs": [],
      "source": [
        "from numpy import moveaxis\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_el = np.inf\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 300       # how many epochs to run for\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "args['patience'] = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NydOdPMajEfT"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        # [(W−K+2P)/S]+1\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(7, stride=7),\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),# 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False), # 8\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),# 4\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 0, bias=False),#14\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**3 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        # H_out ​= (H_in​−1)*stride[0] − 2×padding[0] + dilation[0]×(kernel_size[0]−1) + output_padding[0] + 1\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4), #10\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4), #13\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h, 4),# 16\n",
        "            nn.BatchNorm2d(self.dim_h),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h, 3, 4, 2, 1),# 32\n",
        "            nn.UpsamplingBilinear2d(scale_factor=7),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**3, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def DeepSMOTE_train(X_train, y_train, one_hot = False):\n",
        "  from torch.utils.data import TensorDataset\n",
        "  import os\n",
        "\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #X_train = X_train.astype('float32') / 255.\n",
        "  \n",
        "  batch_size = args['batch_size']\n",
        "  patience = args['patience']\n",
        "  encoder = Encoder(args)\n",
        "  decoder = Decoder(args)\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  print(device)\n",
        "  decoder = decoder.to(device)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "  #decoder loss function\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  num_workers = 0\n",
        "\n",
        "  #torch.Tensor returns float so if want long then use torch.tensor\n",
        "  tensor_x = torch.from_numpy(X_train.copy())#torch.Tensor(X_train)\n",
        "  tensor_y = torch.tensor(y_train,dtype=torch.long)\n",
        "  mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "  train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "      batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "  best_loss = np.inf\n",
        "\n",
        "  enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "  dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "  for epoch in range(args['epochs']):\n",
        "      train_loss = 0.0\n",
        "      tmse_loss = 0.0\n",
        "      tdiscr_loss = 0.0\n",
        "      # train for one epoch -- set nets to train mode\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "  \n",
        "      for images,labs in train_loader:\n",
        "      \n",
        "          # zero gradients for each batch\n",
        "          encoder.zero_grad()\n",
        "          decoder.zero_grad()\n",
        "          images, labs = images.to(device), labs.to(device)\n",
        "          labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "          # run images\n",
        "          z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "      \n",
        "          x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "          mse = criterion(x_hat,images)\n",
        "                  \n",
        "          resx = []\n",
        "          resy = []\n",
        "      \n",
        "          tc = np.random.choice(num_classes,1)\n",
        "          #tc = 9\n",
        "          xbeg = X_train[y_train == tc]\n",
        "          ybeg = y_train[y_train == tc] \n",
        "          xlen = len(xbeg)\n",
        "          nsamp = min(xlen, 100)\n",
        "          ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "          xclass = xbeg[ind]\n",
        "          yclass = ybeg[ind]\n",
        "      \n",
        "          xclen = len(xclass)\n",
        "          xcminus = np.arange(1,xclen)\n",
        "          \n",
        "          xcplus = np.append(xcminus,0)\n",
        "          xcnew = (xclass[[xcplus],:])\n",
        "          xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "      \n",
        "          xcnew = torch.Tensor(xcnew)\n",
        "          xcnew = xcnew.to(device)\n",
        "      \n",
        "          #encode xclass to feature space\n",
        "          xclass = torch.Tensor(xclass)\n",
        "          xclass = xclass.to(device)\n",
        "          xclass = encoder(xclass)\n",
        "      \n",
        "          xclass = xclass.detach().cpu().numpy()\n",
        "      \n",
        "          xc_enc = (xclass[[xcplus],:])\n",
        "          xc_enc = np.squeeze(xc_enc)\n",
        "      \n",
        "          xc_enc = torch.Tensor(xc_enc)\n",
        "          xc_enc = xc_enc.to(device)\n",
        "          \n",
        "          ximg = decoder(xc_enc)\n",
        "          \n",
        "          mse2 = criterion(ximg,xcnew)\n",
        "      \n",
        "          comb_loss = mse2 + mse\n",
        "          comb_loss.backward()\n",
        "      \n",
        "          enc_optim.step()\n",
        "          dec_optim.step()\n",
        "      \n",
        "          train_loss += comb_loss.item()*images.size(0)\n",
        "          tmse_loss += mse.item()*images.size(0)\n",
        "          tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "      train_loss = train_loss/len(train_loader)\n",
        "      tmse_loss = tmse_loss/len(train_loader)\n",
        "      tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "      print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "              train_loss,tmse_loss,tdiscr_loss))\n",
        "      \n",
        "  \n",
        "  \n",
        "      #store the best encoder and decoder models\n",
        "      #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "      #necessary for illustration purposes\n",
        "      if train_loss < best_loss:\n",
        "          print('Saving..')\n",
        "          patience = args['patience']\n",
        "          path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_enc.pth'\n",
        "          path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_dec.pth'\n",
        "        \n",
        "          torch.save(encoder.state_dict(), path_enc)\n",
        "          torch.save(decoder.state_dict(), path_dec)\n",
        "  \n",
        "          best_loss = train_loss\n",
        "      else:\n",
        "          patience = patience - 1\n",
        "\n",
        "      if patience == 0:\n",
        "          print('Out of patience. \\n')\n",
        "          break\n",
        "\n",
        "def DeepSMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  batch_size = args['batch_size']\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #Generate artificial images\n",
        "  import torch\n",
        "  np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "  #path on the computer where the models are stored\n",
        "  modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/'\n",
        "\n",
        "  path_enc = modpth + '/bst_enc.pth'\n",
        "  path_dec = modpth + '/bst_dec.pth'\n",
        "  \n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  encoder = Encoder(args)\n",
        "  encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  decoder = Decoder(args)\n",
        "  decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "  decoder = decoder.to(device)\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  resx = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      #encode xclass to feature space\n",
        "      xclass = torch.Tensor(xclass)\n",
        "      xclass = xclass.to(device)\n",
        "      xclass = encoder(xclass)\n",
        "          \n",
        "      xclass = xclass.detach().cpu().numpy()\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n == 0:\n",
        "        continue\n",
        "#        resx2 = []\n",
        "#        resy2 = []\n",
        "#        for j in range(batch_size, n+batch_size+1, batch_size):\n",
        "#          if j <= n:\n",
        "#            batch_size_max = batch_size\n",
        "#          elif n % batch_size != 0:\n",
        "#            batch_size_max = n%batch_size\n",
        "#          else:\n",
        "#            break\n",
        "#          xsamp, ysamp = G_SM(xclass,yclass,batch_size_max,i)\n",
        "      xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "  \n",
        "      \"\"\"to generate samples for resnet\"\"\"   \n",
        "      xsamp = torch.Tensor(xsamp)\n",
        "      xsamp = xsamp.to(device)\n",
        "      ximg = decoder(xsamp)\n",
        "\n",
        "      ximn = ximg.detach().cpu().numpy()\n",
        "#        resx2.append(ximn)\n",
        "#        resy2.append(ysamp)\n",
        "#        \n",
        "#        resx2 = np.vstack(resx2)\n",
        "#        resy2 = np.hstack(resy2)\n",
        "      resx.append(ximn)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx1 = np.vstack(resx)\n",
        "  resy1 = np.hstack(resy)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = X_train.reshape(X_train.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "  X_train = moveaxis(X_train, 1, 3)\n",
        "  X_train = X_train * max_el\n",
        "  return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6qneWL_Bs2U"
      },
      "outputs": [],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr1jnSM7yzJc"
      },
      "outputs": [],
      "source": [
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1aAQBmiy0K"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['balanced_acc'])\n",
        "plt.plot(hst2.history['val_balanced_acc'])\n",
        "plt.title('model balance_acc after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UswA0co2y1wl",
        "iDRWiTnO0MGh",
        "eaK4zbtoaAaC",
        "3K908bbiYwbS",
        "kE8Ziq-BlEP4",
        "RcRGeofw-8tK",
        "cNBXx28B9yGu",
        "0jrJ33lUDkCM",
        "B2PgksTFkOAq"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}